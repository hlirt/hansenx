[
  {
    "objectID": "Cluster.html#data-preparation",
    "href": "Cluster.html#data-preparation",
    "title": "Weekday vs. Weekend Travel Patterns",
    "section": "Data Preparation",
    "text": "Data Preparation\nBuilding on the observation in Part 1 that there are significant differences in pick-up time periods between weekdays and weekends, I refined the scope of analysis by focusing on Uber trip data from January 2023. To investigate potential differences in travel patterns, I first filtered the January data from the full 2023 dataset (There are still 13,000,000 rows of data…) and categorized trips into weekdays and weekends based on the time_type.\nThis classification sets the foundation for clustering analysis, which examines key trip characteristics—including trip miles, fare, trip duration, and cost per mile—to provide deeper insights into how travel behaviors vary between weekdays and weekends.",
    "crumbs": [
      "Weekday vs. Weekend Travel Patterns"
    ]
  },
  {
    "objectID": "Cluster.html#clustering-analysis-kmeans",
    "href": "Cluster.html#clustering-analysis-kmeans",
    "title": "Weekday vs. Weekend Travel Patterns",
    "section": "Clustering Analysis (KMeans)",
    "text": "Clustering Analysis (KMeans)\nIn the Uber dataset, I aggregate by pickup_location and time_type then extract relevant statistical features.\nFirst, I aggregate the data by pickup_location and time_type, and select fare, trip_miles, trip_duration_min, and cost_per_mile as key features. I then calculate the average values of these features along with the total count for each zone.\nNext, a K-means clustering analysis is performed, dividing the data into 5 distinct clusters to uncover variations in travel patterns across different areas and time types.\n\n\nWeekday Analysis\n\nFare: The average fare remains moderate, with Cluster 3 showing the lowest average fare at $17.99, reflecting a dominance of short-distance commuting trips. In contrast, Cluster 2 has a higher average fare of $49.48, suggesting longer, inter-district trips.\nTrip Miles: Distances vary significantly across clusters. Cluster 2 covers the longest distances, averaging 14.44 miles, indicative of inter-district commuting. Cluster 3, with 4.39 miles, highlights short-distance urban travel patterns.\nDuration: Average trip durations follow the distance patterns, with Cluster 2 taking the longest at 35.57 minutes and Cluster 3 being the shortest at 16.12 minutes, emphasizing the efficiency of weekday commutes.\nTrip Count: The highest trip volumes are recorded in Cluster 4 (5,611,508 trips) and Cluster 3 (2,701,924 trips), reflecting peak-hour commuting demand in urban areas.\n\nWeekend Analysis\n\nFare: Average fares slightly increase compared to weekdays, reflecting higher demand for leisure and entertainment trips. Cluster 4 has a fare of $23.08, while Cluster 3 averages $18.34.\nTrip Miles: Distances are generally shorter during weekends, with Cluster 4 covering an average of 4.35 miles, consistent with localized leisure activities. However, Cluster 2 covers longer distances, averaging 14.50 miles, suggesting inter-district travel.\nDuration: Travel times are longer, with Cluster 2 averaging 31.83 minutes, indicating more relaxed travel patterns for leisure activities. Shorter trips, such as those in Cluster 3 (15.66 minutes), highlight quick, intra-city movements.\nTrip Count: Although total trip volumes decrease compared to weekdays, Cluster 4 maintains a high volume of 1,513,730 trips, suggesting steady demand in central areas during weekends.",
    "crumbs": [
      "Weekday vs. Weekend Travel Patterns"
    ]
  },
  {
    "objectID": "Cluster.html#weekday-vs.-weekendaltair",
    "href": "Cluster.html#weekday-vs.-weekendaltair",
    "title": "Weekday vs. Weekend Travel Patterns",
    "section": "Weekday vs. Weekend（Altair）",
    "text": "Weekday vs. Weekend（Altair）\n\n\nWeekday: On weekdays, Uber trips in NYC are dominated by short-distance, high-frequency commuting patterns, particularly in Manhattan and central business districts. Clusters 3 and 4 capture the highest trip volumes, reflecting commuter demand during peak hours. Meanwhile, peripheral areas like Staten Island and the Bronx (Cluster 2) show longer-distance trips with higher fares, likely serving inter-district commuters traveling to and from residential neighborhoods.\nWeekend: During weekends, Uber trips shift towards leisure and recreational travel, characterized by higher fares per mile and longer trip durations. Cluster 4 highlights shorter-distance trips within city limits, focusing on entertainment and social activities. Conversely, Cluster 2 continues to reflect longer-distance travel patterns, potentially for day trips or special events, while local areas (Cluster 3) sustain moderate volumes for errands and social gatherings.\nThis spatial distribution difference highlights the shift in urban activity patterns between weekdays and weekends.",
    "crumbs": [
      "Weekday vs. Weekend Travel Patterns"
    ]
  },
  {
    "objectID": "ExploratoryAnalysisUberTrips.html#data-preparation",
    "href": "ExploratoryAnalysisUberTrips.html#data-preparation",
    "title": "Exploratory Analysis of Uber Trips in NYC",
    "section": "Data Preparation",
    "text": "Data Preparation\nI began by downloading the 2023 ride-hailing and taxi trip data for New York City from NYC Taxi & Limousine Commission. The dataset included trips from multiple service providers, such as Lyft, Yellow Taxi, and Uber. To focus specifically on Uber trips, I filtered the raw data to isolate records related only to Uber. Given the massive size of the dataset (over 100,000,000 rows), I stored and processed the data in .parquet format to enhance efficiency. Additionally, I leveraged Dask DataFrame to handle the large-scale data and support subsequent analyses.\nNext, I cleaned and refined the data, removing irrelevant information and retaining only the fields essential for analysis—such as location data (pickup and drop-off location), trip duration, and passenger fares.\nFinally, for spatial analysis, I loaded the taxi_zones.shp file, also obtained from NYC Taxi & Limousine Commission. This shapefile served as a spatial reference, enabling me to link trip data to specific geographic zones within New York City.",
    "crumbs": [
      "Exploratory Analysis of Uber Trips in NYC"
    ]
  },
  {
    "objectID": "ExploratoryAnalysisUberTrips.html#travel-time-period-analysis-panel-holoviews-hvplot",
    "href": "ExploratoryAnalysisUberTrips.html#travel-time-period-analysis-panel-holoviews-hvplot",
    "title": "Exploratory Analysis of Uber Trips in NYC",
    "section": "Travel Time Period Analysis (Panel + HoloViews + hvplot)",
    "text": "Travel Time Period Analysis (Panel + HoloViews + hvplot)\nI first analyzed the distribution of Uber trip durations across different weekdays. From the chart, it can be observed that trip durations during weekdays exhibit greater variability, with notably higher medians on Thursday and Friday. In contrast, trip durations on weekends are more concentrated and shorter, which may reflect that weekday trips are primarily driven by commuting needs, resulting in longer and more dispersed durations, whereas weekend trips are likely dominated by shorter leisure travel, leading to more consistent and shorter durations. \nI visualized the time-series distribution of Uber trip demand based on different weekdays. The results reveal distinct patterns between weekdays and weekends. For instance, Sundays exhibit a peak demand at midnight (0:00), likely reflecting late-night activities. In contrast, Mondays show a peak around 8:00 AM, aligning with typical weekday commuting hours. \nSo I further look at the relationship between weekday, weekend and hour by using heatmap. This heatmap shows the distribution of travel demand by day of the week and hour. Weekday demand is higher overall, especially during the morning and evening peaks (6-9 and 17-20), reflecting the commuting demand profile, while the weekend peaks are mainly concentrated at (18-1), showing the travel patterns of nightlife.",
    "crumbs": [
      "Exploratory Analysis of Uber Trips in NYC"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550: Final Project",
    "section": "",
    "text": "Uber is a popular ride-hailing platform that connects passengers with drivers through its mobile app. Unlike traditional taxi services, Uber does not own any vehicles. Instead, it operates as a digital marketplace where riders can book trips with independent drivers using their personal vehicles. For drivers, Uber offers a flexible earning opportunity, while for passengers, it provides a convenient and often more affordable transportation option.\nThis project focuses on the exploration and visualization of Uber trips in New York City during 2023. It aims to provide actionable insights into spatial and temporal trip patterns, travel demand fluctuations, and predictions of Uber trips in different regions. These insights can help Uber drivers identify high-demand areas and improve their efficiency. The data used in this project primarily come from NYC Taxi & Limousine Commission, OpenStreetMap, and the United States Census Bureau, providing detailed information about trip origins, destinations, fares, trip distances, durations, and demographics.\nThis project is divided into 3 main parts:\n\nExploratory Analysis of Uber Trips in NYC\nSpatial Analysis of High-Demand Zones\nUber Trips Prediction for Drivers in Manhattan",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "index.html#exploring-visualizing-and-predicting-uber-trips-in-nyc",
    "href": "index.html#exploring-visualizing-and-predicting-uber-trips-in-nyc",
    "title": "MUSA 550: Final Project",
    "section": "",
    "text": "Uber is a popular ride-hailing platform that connects passengers with drivers through its mobile app. Unlike traditional taxi services, Uber does not own any vehicles. Instead, it operates as a digital marketplace where riders can book trips with independent drivers using their personal vehicles. For drivers, Uber offers a flexible earning opportunity, while for passengers, it provides a convenient and often more affordable transportation option.\nThis project focuses on the exploration and visualization of Uber trips in New York City during 2023. It aims to provide actionable insights into spatial and temporal trip patterns, travel demand fluctuations, and predictions of Uber trips in different regions. These insights can help Uber drivers identify high-demand areas and improve their efficiency. The data used in this project primarily come from NYC Taxi & Limousine Commission, OpenStreetMap, and the United States Census Bureau, providing detailed information about trip origins, destinations, fares, trip distances, durations, and demographics.\nThis project is divided into 3 main parts:\n\nExploratory Analysis of Uber Trips in NYC\nSpatial Analysis of High-Demand Zones\nUber Trips Prediction for Drivers in Manhattan",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "prediction.html",
    "href": "prediction.html",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "",
    "text": "Based on the analysis in Part 2, which revealed significant differences in travel patterns between weekdays and weekends, I will now develop a machine learning model to predict Uber demand across different zones. This prediction aims to assist drivers in identifying high-demand locations more effectively. The final dataset contains 48,135 rows",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "prediction.html#identify-zones-with-the-highest-uber-trips",
    "href": "prediction.html#identify-zones-with-the-highest-uber-trips",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Identify Zones with the Highest Uber Trips",
    "text": "Identify Zones with the Highest Uber Trips\nTo gain insights into the distribution of Uber trip volumes across New York City, I visualized trips based on their geographic locations. \nThis map highlights the spatial distribution of Uber demand in different areas of New York City. The highest demand is concentrated in Manhattan’s core, parts of Brooklyn, and around JFK Airport—particularly in areas with dense commercial and entertainment activities. In contrast, lower demand is observed in peripheral regions, such as eastern Queens and Staten Island. Building on these observations, I will focus my subsequent analysis and demand forecasting on the Manhattan area.",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "prediction.html#feature-engineering",
    "href": "prediction.html#feature-engineering",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nI selected several potential variables that might influence Uber demand. These include facility data, such as restaurants, shops, subway stations, bus stops, schools, and hospitals, as well as demographic data, including median income, poverty population, and commuters by taxi. The data were sourced from OpenStreetMap and the United States Census Bureau.\nBelow, I engineered the features and show the restaurant_density, shop_density, subway_density, hospital_density, school_density, bus_stop_density in the Manhattan area. \nNext, query the Census API to get the demographic features, and visualize the distribution maps for taxi_commuters, median_income, and poverty_population. \n\n\nThen, I calculated the demand at each pickup location with a 1-day, 1-hour, and 2-hour lag, capturing short-term and daily variations in demand trends. These lag features help analysis the temporal patterns of Uber trip demand in Manhattan.\nLast, a correlation matrix was created to examined the correlation between those numerical variables. \n`",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "prediction.html#random-forest-based-prediction-modeling",
    "href": "prediction.html#random-forest-based-prediction-modeling",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Random Forest-Based Prediction Modeling",
    "text": "Random Forest-Based Prediction Modeling\nAfter conducting a correlation analysis, I split the data into a 70/30% training/test set and developed a Random Forest model.\nThe model achieved a Mean Absolute Error (MAE) of 31.05 and a Test R-squared of 0.90, indicating that it can serve as a relatively reliable predictive model.\n\nMAE = 31.046883748972288\n\n\nTest R-squared = 0.8960920639191985\n\nTo further analyze the results, I plotted a bar chart of feature importance, which revealed that the top three most influential features are cost_per_mile, median_income, and shop_density.",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "prediction.html#validation",
    "href": "prediction.html#validation",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Validation",
    "text": "Validation\nFor the test set, we calculated the predicted count, percent error as well as absolute percent error for each observation.\n \nI visualized the spatial distribution of percentage errors and compared the spatial patterns of actual and predicted demand counts. This predictive model effectively captures the overall demand trends across the zone. \nThe maps illustrate the model’s performance in predicting demand across Manhattan. The first map highlights percentage errors, revealing regions with over-predictions (red) and under-predictions (blue). While most areas show relatively small errors, a few zones, particularly in the southern and northern parts, exhibit larger deviations, suggesting localized inaccuracies.\nThe second and third maps compare the actual and predicted counts, showing that the model effectively captures the overall spatial demand patterns, particularly in high-demand zones. However, discrepancies in certain areas, as reflected by the error map, indicate that the model may struggle with fine-scale variations.\nOverall, the model demonstrates strong predictive performance for general trends but requires adjustments to improve accuracy in specific regions with higher errors.",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  }
]