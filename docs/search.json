[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Eric Delmelle, the instructor for the course.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium.",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot.",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550: Final Project",
    "section": "",
    "text": "Uber is a popular ride-hailing platform that connects passengers with drivers through its mobile app. Unlike traditional taxi services, Uber does not own any vehicles. Instead, it operates as a digital marketplace where riders can book trips with independent drivers using their personal vehicles. For drivers, Uber offers a flexible earning opportunity, while for passengers, it provides a convenient and often more affordable transportation option.\nThis project focuses on the exploration and visualization of Uber trips in New York City during 2023. It aims to provide actionable insights into spatial and temporal trip patterns, travel demand fluctuations, and predictions of Uber trips in different regions. These insights can help Uber drivers identify high-demand areas and improve their efficiency. The data used in this project primarily come from NYC Taxi & Limousine Commission, OpenStreetMap, and the United States Census Bureau, providing detailed information about trip origins, destinations, fares, trip distances, durations, and demographics.\nThis project is divided into 3 main parts:\n\nExploratory Analysis of Uber Trips in NYC\nSpatial Analysis of High-Demand Zones\nUber Trips Prediction for Drivers in Manhattan",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550: Final Project",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "analysis/assignment-3.html",
    "href": "analysis/assignment-3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# See lots of columns\npd.options.display.max_columns = 999\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n%matplotlib inline\nThis assignment will contain two parts:",
    "crumbs": [
      "Analysis",
      "Assignment 3"
    ]
  },
  {
    "objectID": "analysis/assignment-3.html#part-1-exploring-evictions-and-code-violations-in-philadelphia",
    "href": "analysis/assignment-3.html#part-1-exploring-evictions-and-code-violations-in-philadelphia",
    "title": "Assignment 3",
    "section": "Part 1: Exploring Evictions and Code Violations in Philadelphia",
    "text": "Part 1: Exploring Evictions and Code Violations in Philadelphia\nIn this assignment, we’ll explore spatial trends evictions in Philadelphia using data from the Eviction Lab and building code violations using data from OpenDataPhilly.\nWe’ll be exploring the idea that evictions can occur as retaliation against renters for reporting code violations. Spatial correlations between evictions and code violations from the City’s Licenses and Inspections department can offer some insight into this question.\nA couple of interesting background readings: - HuffPost article - PlanPhilly article",
    "crumbs": [
      "Analysis",
      "Assignment 3"
    ]
  },
  {
    "objectID": "analysis/assignment-3.html#explore-eviction-lab-data",
    "href": "analysis/assignment-3.html#explore-eviction-lab-data",
    "title": "Assignment 3",
    "section": "1.1 Explore Eviction Lab Data",
    "text": "1.1 Explore Eviction Lab Data\nThe Eviction Lab built the first national database for evictions. If you aren’t familiar with the project, you can explore their website: https://evictionlab.org/\n\n1.1.1 Read data using geopandas\nThe first step is to read the eviction data by census tract using geopandas. The data for all of Pennsylvania by census tract is available in the data/ folder in a GeoJSON format.\nLoad the data file “PA-tracts.geojson” using geopandas\nNote: If you’d like to see all columns in the data frame, you can increase the max number of columns using pandas display options:\n\npa_tracts = gpd.read_file(\"data/PA-tracts.geojson\")\n\n\npa_tracts.head()\n\n\n\n\n\n\n\n\nGEOID\nwest\nsouth\neast\nnorth\nn\npl\np-00\npr-00\nroh-00\npro-00\nmgr-00\nmhi-00\nmpv-00\nrb-00\npw-00\npaa-00\nph-00\npai-00\npa-00\npnp-00\npm-00\npo-00\nef-00\ne-00\ner-00\nefr-00\nlf-00\nimputed-00\nsubbed-00\np-01\npr-01\nroh-01\npro-01\nmgr-01\nmhi-01\nmpv-01\nrb-01\npw-01\npaa-01\nph-01\npai-01\npa-01\npnp-01\npm-01\npo-01\nef-01\ne-01\ner-01\nefr-01\nlf-01\nimputed-01\nsubbed-01\np-02\npr-02\nroh-02\npro-02\nmgr-02\nmhi-02\nmpv-02\nrb-02\npw-02\npaa-02\nph-02\npai-02\npa-02\npnp-02\npm-02\npo-02\nef-02\ne-02\ner-02\nefr-02\nlf-02\nimputed-02\nsubbed-02\np-03\npr-03\nroh-03\npro-03\nmgr-03\nmhi-03\nmpv-03\nrb-03\npw-03\npaa-03\nph-03\npai-03\npa-03\npnp-03\npm-03\npo-03\nef-03\ne-03\ner-03\nefr-03\nlf-03\nimputed-03\nsubbed-03\np-04\npr-04\nroh-04\npro-04\nmgr-04\nmhi-04\nmpv-04\nrb-04\npw-04\npaa-04\nph-04\npai-04\npa-04\npnp-04\npm-04\npo-04\nef-04\ne-04\ner-04\nefr-04\nlf-04\nimputed-04\nsubbed-04\np-05\npr-05\nroh-05\npro-05\nmgr-05\nmhi-05\nmpv-05\nrb-05\npw-05\npaa-05\nph-05\npai-05\npa-05\npnp-05\npm-05\npo-05\nef-05\ne-05\ner-05\nefr-05\nlf-05\nimputed-05\nsubbed-05\np-06\npr-06\nroh-06\npro-06\nmgr-06\nmhi-06\nmpv-06\nrb-06\npw-06\npaa-06\nph-06\npai-06\npa-06\npnp-06\npm-06\npo-06\nef-06\ne-06\ner-06\nefr-06\nlf-06\nimputed-06\nsubbed-06\np-07\npr-07\nroh-07\npro-07\nmgr-07\nmhi-07\nmpv-07\nrb-07\npw-07\npaa-07\nph-07\npai-07\npa-07\npnp-07\npm-07\npo-07\nef-07\ne-07\ner-07\nefr-07\nlf-07\nimputed-07\nsubbed-07\np-08\npr-08\nroh-08\npro-08\nmgr-08\nmhi-08\nmpv-08\nrb-08\npw-08\npaa-08\nph-08\npai-08\npa-08\npnp-08\npm-08\npo-08\nef-08\ne-08\ner-08\nefr-08\nlf-08\nimputed-08\nsubbed-08\np-09\npr-09\nroh-09\npro-09\nmgr-09\nmhi-09\nmpv-09\nrb-09\npw-09\npaa-09\nph-09\npai-09\npa-09\npnp-09\npm-09\npo-09\nef-09\ne-09\ner-09\nefr-09\nlf-09\nimputed-09\nsubbed-09\np-10\npr-10\nroh-10\npro-10\nmgr-10\nmhi-10\nmpv-10\nrb-10\npw-10\npaa-10\nph-10\npai-10\npa-10\npnp-10\npm-10\npo-10\nef-10\ne-10\ner-10\nefr-10\nlf-10\nimputed-10\nsubbed-10\np-11\npr-11\nroh-11\npro-11\nmgr-11\nmhi-11\nmpv-11\nrb-11\npw-11\npaa-11\nph-11\npai-11\npa-11\npnp-11\npm-11\npo-11\nef-11\ne-11\ner-11\nefr-11\nlf-11\nimputed-11\nsubbed-11\np-12\npr-12\nroh-12\npro-12\nmgr-12\nmhi-12\nmpv-12\nrb-12\npw-12\npaa-12\nph-12\npai-12\npa-12\npnp-12\npm-12\npo-12\nef-12\ne-12\ner-12\nefr-12\nlf-12\nimputed-12\nsubbed-12\np-13\npr-13\nroh-13\npro-13\nmgr-13\nmhi-13\nmpv-13\nrb-13\npw-13\npaa-13\nph-13\npai-13\npa-13\npnp-13\npm-13\npo-13\nef-13\ne-13\ner-13\nefr-13\nlf-13\nimputed-13\nsubbed-13\np-14\npr-14\nroh-14\npro-14\nmgr-14\nmhi-14\nmpv-14\nrb-14\npw-14\npaa-14\nph-14\npai-14\npa-14\npnp-14\npm-14\npo-14\nef-14\ne-14\ner-14\nefr-14\nlf-14\nimputed-14\nsubbed-14\np-15\npr-15\nroh-15\npro-15\nmgr-15\nmhi-15\nmpv-15\nrb-15\npw-15\npaa-15\nph-15\npai-15\npa-15\npnp-15\npm-15\npo-15\nef-15\ne-15\ner-15\nefr-15\nlf-15\nimputed-15\nsubbed-15\np-16\npr-16\nroh-16\npro-16\nmgr-16\nmhi-16\nmpv-16\nrb-16\npw-16\npaa-16\nph-16\npai-16\npa-16\npnp-16\npm-16\npo-16\nef-16\ne-16\ner-16\nefr-16\nlf-16\nimputed-16\nsubbed-16\ngeometry\n\n\n\n\n0\n42003412002\n-80.1243\n40.5422\n-80.0640\n40.5890\n4120.02\nAllegheny County, Pennsylvania\n4748.59\n0.88\n58.0\n3.66\n949.31\n89226.06\n195358.33\n22.82\n94.89\n0.84\n0.43\n0.06\n2.83\n0.33\n0.62\n0.00\n1.0\n1.0\n1.73\n1.73\n0.0\n0.0\n0.0\n4748.59\n0.88\n59.0\n3.66\n949.31\n89226.06\n195358.33\n22.82\n94.89\n0.84\n0.43\n0.06\n2.83\n0.33\n0.62\n0.00\n0.0\n0.0\n0.00\n0.00\n0.0\n0.0\n0.0\n4748.59\n0.88\n60.0\n3.66\n949.31\n89226.06\n195358.33\n22.82\n94.89\n0.84\n0.43\n0.06\n2.83\n0.33\n0.62\n0.00\n3.0\n0.0\n0.00\n4.99\n0.0\n0.0\n0.0\n4748.59\n0.88\n61.0\n3.66\n949.31\n89226.06\n195358.33\n22.82\n94.89\n0.84\n0.43\n0.06\n2.83\n0.33\n0.62\n0.00\n0.0\n0.0\n0.00\n0.00\n0.0\n0.0\n0.0\n4748.59\n0.88\n62.0\n3.66\n949.31\n89226.06\n195358.33\n22.82\n94.89\n0.84\n0.43\n0.06\n2.83\n0.33\n0.62\n0.00\n0.0\n0.0\n0.00\n0.00\n0.0\n0.0\n0.0\n4525.07\n0.93\n63.0\n5.83\n799.06\n117867.25\n267511.43\n13.86\n89.33\n1.42\n2.65\n0.00\n6.41\n0.0\n0.00\n0.19\n1.0\n0.0\n0.00\n1.58\n0.0\n0.0\n0.0\n4525.07\n0.93\n65.0\n5.83\n799.06\n117867.25\n267511.43\n13.86\n89.33\n1.42\n2.65\n0.00\n6.41\n0.0\n0.00\n0.19\n0.0\n0.0\n0.00\n0.00\n0.0\n0.0\n0.0\n4525.07\n0.93\n66.0\n5.83\n799.06\n117867.25\n267511.43\n13.86\n89.33\n1.42\n2.65\n0.00\n6.41\n0.0\n0.00\n0.19\n0.0\n0.0\n0.00\n0.00\n0.0\n0.0\n1.0\n4525.07\n0.93\n67.0\n5.83\n799.06\n117867.25\n267511.43\n13.86\n89.33\n1.42\n2.65\n0.00\n6.41\n0.0\n0.00\n0.19\n5.0\n4.0\n5.99\n7.49\n0.0\n0.0\n1.0\n4525.07\n0.93\n68.0\n5.83\n799.06\n117867.25\n267511.43\n13.86\n89.33\n1.42\n2.65\n0.00\n6.41\n0.0\n0.00\n0.19\n2.0\n1.0\n1.47\n2.95\n0.0\n0.0\n1.0\n4865.0\n0.00\n69.0\n4.08\n838.0\n119583.0\n280900.0\n16.9\n91.57\n0.84\n1.03\n0.00\n5.80\n0.12\n0.55\n0.08\n1.0\n0.0\n0.00\n1.45\n0.0\n0.0\n1.0\n5362.0\n0.46\n71.0\n3.28\n1313.0\n118611.0\n302900.0\n50.0\n91.53\n0.50\n0.97\n0.06\n6.94\n0.0\n0.00\n0.0\n2.0\n0.0\n0.00\n2.82\n0.0\n0.0\n1.0\n5362.0\n0.46\n73.0\n3.28\n1313.0\n118611.0\n302900.0\n50.0\n91.53\n0.50\n0.97\n0.06\n6.94\n0.0\n0.00\n0.0\n2.0\n0.0\n0.00\n2.75\n0.0\n0.0\n1.0\n5362.0\n0.46\n75.0\n3.28\n1313.0\n118611.0\n302900.0\n50.0\n91.53\n0.50\n0.97\n0.06\n6.94\n0.0\n0.00\n0.0\n0.0\n0.0\n0.00\n0.00\n0.0\n0.0\n1.0\n5362.0\n0.46\n76.0\n3.28\n1313.0\n118611.0\n302900.0\n50.0\n91.53\n0.50\n0.97\n0.06\n6.94\n0.0\n0.00\n0.0\n1.0\n0.0\n0.00\n1.31\n0.0\n0.0\n1.0\n5362.0\n0.46\n78.0\n3.28\n1313.0\n118611.0\n302900.0\n50.0\n91.53\n0.50\n0.97\n0.06\n6.94\n0.0\n0.00\n0.0\n1.0\n0.0\n0.00\n1.28\n0.0\n0.0\n1.0\n5362.0\n0.46\n80.0\n3.28\n1313.0\n118611.0\n302900.0\n50.0\n91.53\n0.50\n0.97\n0.06\n6.94\n0.0\n0.00\n0.0\n0.0\n0.0\n0.00\n0.00\n1.0\n0.0\n1.0\nMULTIPOLYGON (((-80.06670 40.58401, -80.06655 ...\n\n\n1\n42003413100\n-80.0681\n40.5850\n-79.9906\n40.6143\n4131\nAllegheny County, Pennsylvania\n6771.01\n3.47\n729.0\n28.75\n674.10\n75492.65\n193726.88\n21.39\n91.61\n1.81\n0.94\n0.03\n4.85\n0.01\n0.66\n0.07\n10.0\n4.0\n0.55\n1.37\n0.0\n0.0\n0.0\n6771.01\n3.47\n724.0\n28.75\n674.10\n75492.65\n193726.88\n21.39\n91.61\n1.81\n0.94\n0.03\n4.85\n0.01\n0.66\n0.07\n16.0\n11.0\n1.52\n2.21\n0.0\n0.0\n0.0\n6771.01\n3.47\n719.0\n28.75\n674.10\n75492.65\n193726.88\n21.39\n91.61\n1.81\n0.94\n0.03\n4.85\n0.01\n0.66\n0.07\n22.0\n17.0\n2.37\n3.06\n0.0\n0.0\n0.0\n6771.01\n3.47\n713.0\n28.75\n674.10\n75492.65\n193726.88\n21.39\n91.61\n1.81\n0.94\n0.03\n4.85\n0.01\n0.66\n0.07\n14.0\n10.0\n1.40\n1.96\n0.0\n0.0\n0.0\n6771.01\n3.47\n708.0\n28.75\n674.10\n75492.65\n193726.88\n21.39\n91.61\n1.81\n0.94\n0.03\n4.85\n0.01\n0.66\n0.07\n19.0\n8.0\n1.13\n2.68\n0.0\n0.0\n0.0\n5648.32\n2.06\n703.0\n31.24\n830.01\n86853.22\n278013.85\n26.67\n89.57\n1.87\n2.30\n0.00\n3.88\n0.0\n2.38\n0.00\n31.0\n12.0\n1.71\n4.41\n0.0\n0.0\n0.0\n5648.32\n2.06\n697.0\n31.24\n830.01\n86853.22\n278013.85\n26.67\n89.57\n1.87\n2.30\n0.00\n3.88\n0.0\n2.38\n0.00\n22.0\n14.0\n2.01\n3.15\n0.0\n0.0\n0.0\n5648.32\n2.06\n692.0\n31.24\n830.01\n86853.22\n278013.85\n26.67\n89.57\n1.87\n2.30\n0.00\n3.88\n0.0\n2.38\n0.00\n34.0\n4.0\n0.58\n4.91\n0.0\n0.0\n1.0\n5648.32\n2.06\n687.0\n31.24\n830.01\n86853.22\n278013.85\n26.67\n89.57\n1.87\n2.30\n0.00\n3.88\n0.0\n2.38\n0.00\n43.0\n6.0\n0.87\n6.26\n0.0\n0.0\n1.0\n5648.32\n2.06\n681.0\n31.24\n830.01\n86853.22\n278013.85\n26.67\n89.57\n1.87\n2.30\n0.00\n3.88\n0.0\n2.38\n0.00\n24.0\n3.0\n0.44\n3.52\n0.0\n0.0\n1.0\n6609.0\n2.68\n676.0\n26.19\n865.0\n97264.0\n271600.0\n23.3\n87.80\n1.72\n1.74\n0.15\n7.49\n0.11\n0.86\n0.12\n37.0\n1.0\n0.15\n5.47\n0.0\n0.0\n1.0\n7223.0\n4.26\n687.0\n27.37\n943.0\n114036.0\n286600.0\n21.7\n77.24\n3.88\n1.27\n0.00\n16.02\n0.0\n1.59\n0.0\n49.0\n3.0\n0.44\n7.14\n0.0\n0.0\n1.0\n7223.0\n4.26\n697.0\n27.37\n943.0\n114036.0\n286600.0\n21.7\n77.24\n3.88\n1.27\n0.00\n16.02\n0.0\n1.59\n0.0\n29.0\n5.0\n0.72\n4.16\n0.0\n0.0\n1.0\n7223.0\n4.26\n708.0\n27.37\n943.0\n114036.0\n286600.0\n21.7\n77.24\n3.88\n1.27\n0.00\n16.02\n0.0\n1.59\n0.0\n23.0\n3.0\n0.42\n3.25\n0.0\n0.0\n1.0\n7223.0\n4.26\n718.0\n27.37\n943.0\n114036.0\n286600.0\n21.7\n77.24\n3.88\n1.27\n0.00\n16.02\n0.0\n1.59\n0.0\n21.0\n2.0\n0.28\n2.92\n0.0\n0.0\n1.0\n7223.0\n4.26\n729.0\n27.37\n943.0\n114036.0\n286600.0\n21.7\n77.24\n3.88\n1.27\n0.00\n16.02\n0.0\n1.59\n0.0\n16.0\n2.0\n0.27\n2.20\n0.0\n0.0\n1.0\n7223.0\n4.26\n739.0\n27.37\n943.0\n114036.0\n286600.0\n21.7\n77.24\n3.88\n1.27\n0.00\n16.02\n0.0\n1.59\n0.0\n12.0\n2.0\n0.27\n1.62\n1.0\n0.0\n1.0\nMULTIPOLYGON (((-80.06806 40.61254, -80.05452 ...\n\n\n2\n42003413300\n-80.0657\n40.5527\n-80.0210\n40.5721\n4133\nAllegheny County, Pennsylvania\n5044.59\n2.99\n119.0\n6.68\n938.08\n60019.14\n131521.90\n28.36\n97.15\n0.37\n0.49\n0.12\n1.61\n0.00\n0.26\n0.00\n2.0\n1.0\n0.84\n1.68\n0.0\n0.0\n0.0\n5044.59\n2.99\n125.0\n6.68\n938.08\n60019.14\n131521.90\n28.36\n97.15\n0.37\n0.49\n0.12\n1.61\n0.00\n0.26\n0.00\n2.0\n2.0\n1.60\n1.60\n0.0\n0.0\n0.0\n5044.59\n2.99\n131.0\n6.68\n938.08\n60019.14\n131521.90\n28.36\n97.15\n0.37\n0.49\n0.12\n1.61\n0.00\n0.26\n0.00\n0.0\n0.0\n0.00\n0.00\n0.0\n0.0\n0.0\n5044.59\n2.99\n137.0\n6.68\n938.08\n60019.14\n131521.90\n28.36\n97.15\n0.37\n0.49\n0.12\n1.61\n0.00\n0.26\n0.00\n4.0\n0.0\n0.00\n2.92\n0.0\n0.0\n0.0\n5044.59\n2.99\n143.0\n6.68\n938.08\n60019.14\n131521.90\n28.36\n97.15\n0.37\n0.49\n0.12\n1.61\n0.00\n0.26\n0.00\n2.0\n0.0\n0.00\n1.40\n0.0\n0.0\n0.0\n4562.01\n2.50\n149.0\n4.97\n680.77\n75425.61\n162394.01\n23.93\n93.87\n1.56\n0.81\n0.00\n3.76\n0.0\n0.00\n0.00\n4.0\n1.0\n0.67\n2.69\n0.0\n0.0\n0.0\n4562.01\n2.50\n154.0\n4.97\n680.77\n75425.61\n162394.01\n23.93\n93.87\n1.56\n0.81\n0.00\n3.76\n0.0\n0.00\n0.00\n5.0\n5.0\n3.24\n3.24\n0.0\n0.0\n0.0\n4562.01\n2.50\n160.0\n4.97\n680.77\n75425.61\n162394.01\n23.93\n93.87\n1.56\n0.81\n0.00\n3.76\n0.0\n0.00\n0.00\n1.0\n0.0\n0.00\n0.62\n0.0\n0.0\n1.0\n4562.01\n2.50\n166.0\n4.97\n680.77\n75425.61\n162394.01\n23.93\n93.87\n1.56\n0.81\n0.00\n3.76\n0.0\n0.00\n0.00\n2.0\n0.0\n0.00\n1.20\n0.0\n0.0\n1.0\n4562.01\n2.50\n172.0\n4.97\n680.77\n75425.61\n162394.01\n23.93\n93.87\n1.56\n0.81\n0.00\n3.76\n0.0\n0.00\n0.00\n3.0\n1.0\n0.58\n1.74\n0.0\n0.0\n1.0\n4742.0\n2.54\n178.0\n9.59\n936.0\n84688.0\n170400.0\n25.0\n93.72\n0.97\n0.78\n0.11\n3.52\n0.00\n0.76\n0.15\n6.0\n2.0\n1.12\n3.37\n0.0\n0.0\n1.0\n4744.0\n2.52\n182.0\n10.96\n780.0\n78630.0\n174200.0\n20.7\n91.74\n1.52\n2.21\n0.19\n3.39\n0.0\n0.95\n0.0\n0.0\n0.0\n0.00\n0.00\n0.0\n0.0\n1.0\n4744.0\n2.52\n187.0\n10.96\n780.0\n78630.0\n174200.0\n20.7\n91.74\n1.52\n2.21\n0.19\n3.39\n0.0\n0.95\n0.0\n6.0\n1.0\n0.54\n3.21\n0.0\n0.0\n1.0\n4744.0\n2.52\n191.0\n10.96\n780.0\n78630.0\n174200.0\n20.7\n91.74\n1.52\n2.21\n0.19\n3.39\n0.0\n0.95\n0.0\n1.0\n0.0\n0.00\n0.52\n0.0\n0.0\n1.0\n4744.0\n2.52\n195.0\n10.96\n780.0\n78630.0\n174200.0\n20.7\n91.74\n1.52\n2.21\n0.19\n3.39\n0.0\n0.95\n0.0\n1.0\n1.0\n0.51\n0.51\n0.0\n0.0\n1.0\n4744.0\n2.52\n200.0\n10.96\n780.0\n78630.0\n174200.0\n20.7\n91.74\n1.52\n2.21\n0.19\n3.39\n0.0\n0.95\n0.0\n2.0\n0.0\n0.00\n1.00\n0.0\n0.0\n1.0\n4744.0\n2.52\n204.0\n10.96\n780.0\n78630.0\n174200.0\n20.7\n91.74\n1.52\n2.21\n0.19\n3.39\n0.0\n0.95\n0.0\n4.0\n1.0\n0.49\n1.96\n1.0\n0.0\n1.0\nMULTIPOLYGON (((-80.03822 40.55349, -80.04368 ...\n\n\n3\n42003416000\n-79.8113\n40.5440\n-79.7637\n40.5630\n4160\nAllegheny County, Pennsylvania\n1775.93\n4.99\n121.0\n15.30\n557.90\n39073.61\n77316.48\n17.60\n99.50\n0.06\n0.11\n0.00\n0.00\n0.00\n0.33\n0.00\n1.0\n1.0\n0.83\n0.83\n0.0\n0.0\n0.0\n1775.93\n4.99\n123.0\n15.30\n557.90\n39073.61\n77316.48\n17.60\n99.50\n0.06\n0.11\n0.00\n0.00\n0.00\n0.33\n0.00\n3.0\n3.0\n2.44\n2.44\n0.0\n0.0\n0.0\n1775.93\n4.99\n125.0\n15.30\n557.90\n39073.61\n77316.48\n17.60\n99.50\n0.06\n0.11\n0.00\n0.00\n0.00\n0.33\n0.00\n3.0\n3.0\n2.41\n2.41\n0.0\n0.0\n0.0\n1775.93\n4.99\n127.0\n15.30\n557.90\n39073.61\n77316.48\n17.60\n99.50\n0.06\n0.11\n0.00\n0.00\n0.00\n0.33\n0.00\n4.0\n3.0\n2.37\n3.16\n0.0\n0.0\n0.0\n1775.93\n4.99\n128.0\n15.30\n557.90\n39073.61\n77316.48\n17.60\n99.50\n0.06\n0.11\n0.00\n0.00\n0.00\n0.33\n0.00\n2.0\n1.0\n0.78\n1.56\n0.0\n0.0\n0.0\n1569.13\n7.82\n130.0\n18.14\n546.12\n43434.32\n96418.92\n27.51\n98.74\n0.00\n0.00\n0.13\n0.00\n0.0\n1.13\n0.00\n7.0\n6.0\n4.61\n5.38\n0.0\n0.0\n0.0\n1569.13\n7.82\n132.0\n18.14\n546.12\n43434.32\n96418.92\n27.51\n98.74\n0.00\n0.00\n0.13\n0.00\n0.0\n1.13\n0.00\n7.0\n4.0\n3.03\n5.31\n0.0\n0.0\n0.0\n1569.13\n7.82\n134.0\n18.14\n546.12\n43434.32\n96418.92\n27.51\n98.74\n0.00\n0.00\n0.13\n0.00\n0.0\n1.13\n0.00\n0.0\n0.0\n0.00\n0.00\n0.0\n0.0\n1.0\n1569.13\n7.82\n135.0\n18.14\n546.12\n43434.32\n96418.92\n27.51\n98.74\n0.00\n0.00\n0.13\n0.00\n0.0\n1.13\n0.00\n2.0\n2.0\n1.48\n1.48\n0.0\n0.0\n1.0\n1569.13\n7.82\n137.0\n18.14\n546.12\n43434.32\n96418.92\n27.51\n98.74\n0.00\n0.00\n0.13\n0.00\n0.0\n1.13\n0.00\n5.0\n4.0\n2.92\n3.64\n0.0\n0.0\n1.0\n1636.0\n5.00\n139.0\n17.48\n766.0\n45491.0\n95100.0\n26.4\n99.08\n0.18\n0.61\n0.00\n0.06\n0.00\n0.06\n0.00\n6.0\n4.0\n2.88\n4.32\n0.0\n0.0\n1.0\n1643.0\n6.16\n141.0\n23.59\n680.0\n48438.0\n94900.0\n19.7\n99.45\n0.00\n0.00\n0.00\n0.00\n0.0\n0.55\n0.0\n1.0\n1.0\n0.71\n0.71\n0.0\n0.0\n1.0\n1643.0\n6.16\n144.0\n23.59\n680.0\n48438.0\n94900.0\n19.7\n99.45\n0.00\n0.00\n0.00\n0.00\n0.0\n0.55\n0.0\n3.0\n3.0\n2.09\n2.09\n0.0\n0.0\n1.0\n1643.0\n6.16\n146.0\n23.59\n680.0\n48438.0\n94900.0\n19.7\n99.45\n0.00\n0.00\n0.00\n0.00\n0.0\n0.55\n0.0\n4.0\n2.0\n1.37\n2.74\n0.0\n0.0\n1.0\n1643.0\n6.16\n148.0\n23.59\n680.0\n48438.0\n94900.0\n19.7\n99.45\n0.00\n0.00\n0.00\n0.00\n0.0\n0.55\n0.0\n5.0\n2.0\n1.35\n3.37\n0.0\n0.0\n1.0\n1643.0\n6.16\n151.0\n23.59\n680.0\n48438.0\n94900.0\n19.7\n99.45\n0.00\n0.00\n0.00\n0.00\n0.0\n0.55\n0.0\n3.0\n2.0\n1.33\n1.99\n0.0\n0.0\n1.0\n1643.0\n6.16\n153.0\n23.59\n680.0\n48438.0\n94900.0\n19.7\n99.45\n0.00\n0.00\n0.00\n0.00\n0.0\n0.55\n0.0\n1.0\n1.0\n0.65\n0.65\n1.0\n0.0\n1.0\nMULTIPOLYGON (((-79.76595 40.55092, -79.76542 ...\n\n\n4\n42003417200\n-79.7948\n40.5341\n-79.7642\n40.5443\n4172\nAllegheny County, Pennsylvania\n1428.03\n11.95\n321.0\n48.27\n409.00\n34306.14\n66400.38\n22.10\n98.46\n0.63\n0.28\n0.07\n0.07\n0.00\n0.35\n0.14\n9.0\n7.0\n2.18\n2.80\n0.0\n0.0\n0.0\n1428.03\n11.95\n322.0\n48.27\n409.00\n34306.14\n66400.38\n22.10\n98.46\n0.63\n0.28\n0.07\n0.07\n0.00\n0.35\n0.14\n12.0\n4.0\n1.24\n3.73\n0.0\n0.0\n0.0\n1428.03\n11.95\n323.0\n48.27\n409.00\n34306.14\n66400.38\n22.10\n98.46\n0.63\n0.28\n0.07\n0.07\n0.00\n0.35\n0.14\n17.0\n9.0\n2.79\n5.27\n0.0\n0.0\n0.0\n1428.03\n11.95\n324.0\n48.27\n409.00\n34306.14\n66400.38\n22.10\n98.46\n0.63\n0.28\n0.07\n0.07\n0.00\n0.35\n0.14\n12.0\n7.0\n2.16\n3.71\n0.0\n0.0\n0.0\n1428.03\n11.95\n325.0\n48.27\n409.00\n34306.14\n66400.38\n22.10\n98.46\n0.63\n0.28\n0.07\n0.07\n0.00\n0.35\n0.14\n12.0\n9.0\n2.77\n3.70\n0.0\n0.0\n0.0\n1345.03\n9.52\n326.0\n41.37\n716.00\n40257.36\n81200.46\n25.00\n90.63\n0.00\n7.43\n0.00\n0.00\n0.0\n1.93\n0.00\n11.0\n11.0\n3.38\n3.38\n0.0\n0.0\n0.0\n1345.03\n9.52\n326.0\n41.37\n716.00\n40257.36\n81200.46\n25.00\n90.63\n0.00\n7.43\n0.00\n0.00\n0.0\n1.93\n0.00\n14.0\n12.0\n3.68\n4.29\n0.0\n0.0\n0.0\n1345.03\n9.52\n327.0\n41.37\n716.00\n40257.36\n81200.46\n25.00\n90.63\n0.00\n7.43\n0.00\n0.00\n0.0\n1.93\n0.00\n1.0\n0.0\n0.00\n0.31\n0.0\n0.0\n1.0\n1345.03\n9.52\n328.0\n41.37\n716.00\n40257.36\n81200.46\n25.00\n90.63\n0.00\n7.43\n0.00\n0.00\n0.0\n1.93\n0.00\n1.0\n0.0\n0.00\n0.30\n0.0\n0.0\n1.0\n1345.03\n9.52\n329.0\n41.37\n716.00\n40257.36\n81200.46\n25.00\n90.63\n0.00\n7.43\n0.00\n0.00\n0.0\n1.93\n0.00\n9.0\n2.0\n0.61\n2.73\n0.0\n0.0\n1.0\n1260.0\n1.32\n330.0\n52.13\n739.0\n39934.0\n96500.0\n29.3\n96.43\n1.19\n0.79\n0.08\n0.08\n0.00\n1.27\n0.16\n11.0\n6.0\n1.82\n3.33\n0.0\n0.0\n1.0\n1253.0\n8.36\n336.0\n46.57\n625.0\n32614.0\n78300.0\n33.0\n97.69\n0.96\n1.36\n0.00\n0.00\n0.0\n0.00\n0.0\n11.0\n5.0\n1.49\n3.27\n0.0\n0.0\n1.0\n1253.0\n8.36\n343.0\n46.57\n625.0\n32614.0\n78300.0\n33.0\n97.69\n0.96\n1.36\n0.00\n0.00\n0.0\n0.00\n0.0\n6.0\n4.0\n1.17\n1.75\n0.0\n0.0\n1.0\n1253.0\n8.36\n349.0\n46.57\n625.0\n32614.0\n78300.0\n33.0\n97.69\n0.96\n1.36\n0.00\n0.00\n0.0\n0.00\n0.0\n6.0\n5.0\n1.43\n1.72\n0.0\n0.0\n1.0\n1253.0\n8.36\n355.0\n46.57\n625.0\n32614.0\n78300.0\n33.0\n97.69\n0.96\n1.36\n0.00\n0.00\n0.0\n0.00\n0.0\n13.0\n8.0\n2.25\n3.66\n0.0\n0.0\n1.0\n1253.0\n8.36\n362.0\n46.57\n625.0\n32614.0\n78300.0\n33.0\n97.69\n0.96\n1.36\n0.00\n0.00\n0.0\n0.00\n0.0\n8.0\n6.0\n1.66\n2.21\n0.0\n0.0\n1.0\n1253.0\n8.36\n368.0\n46.57\n625.0\n32614.0\n78300.0\n33.0\n97.69\n0.96\n1.36\n0.00\n0.00\n0.0\n0.00\n0.0\n7.0\n3.0\n0.82\n1.90\n1.0\n0.0\n1.0\nMULTIPOLYGON (((-79.77114 40.54415, -79.76417 ...\n\n\n\n\n\n\n\n\ntype(pa_tracts)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\n\n1.1.2 Explore and trim the data\nWe will need to trim data to Philadelphia only. Take a look at the data dictionary for the descriptions of the various columns in top-level repository folder: eviction_lab_data_dictionary.txt\nNote: the column names are shortened — see the end of the above file for the abbreviations. The numbers at the end of the columns indicate the years. For example, e-16 is the number of evictions in 2016.\nTake a look at the individual columns and trim to census tracts in Philadelphia. (Hint: Philadelphia is both a city and a county).\n\nphilly_tracts = pa_tracts[pa_tracts['pl'] == 'Philadelphia County, Pennsylvania']\n\n\nphilly_tracts.head()\n\n\n\n\n\n\n\n\nGEOID\nwest\nsouth\neast\nnorth\nn\npl\np-00\npr-00\nroh-00\npro-00\nmgr-00\nmhi-00\nmpv-00\nrb-00\npw-00\npaa-00\nph-00\npai-00\npa-00\npnp-00\npm-00\npo-00\nef-00\ne-00\ner-00\nefr-00\nlf-00\nimputed-00\nsubbed-00\np-01\npr-01\nroh-01\npro-01\nmgr-01\nmhi-01\nmpv-01\nrb-01\npw-01\npaa-01\nph-01\npai-01\npa-01\npnp-01\npm-01\npo-01\nef-01\ne-01\ner-01\nefr-01\nlf-01\nimputed-01\nsubbed-01\np-02\npr-02\nroh-02\npro-02\nmgr-02\nmhi-02\nmpv-02\nrb-02\npw-02\npaa-02\nph-02\npai-02\npa-02\npnp-02\npm-02\npo-02\nef-02\ne-02\ner-02\nefr-02\nlf-02\nimputed-02\nsubbed-02\np-03\npr-03\nroh-03\npro-03\nmgr-03\nmhi-03\nmpv-03\nrb-03\npw-03\npaa-03\nph-03\npai-03\npa-03\npnp-03\npm-03\npo-03\nef-03\ne-03\ner-03\nefr-03\nlf-03\nimputed-03\nsubbed-03\np-04\npr-04\nroh-04\npro-04\nmgr-04\nmhi-04\nmpv-04\nrb-04\npw-04\npaa-04\nph-04\npai-04\npa-04\npnp-04\npm-04\npo-04\nef-04\ne-04\ner-04\nefr-04\nlf-04\nimputed-04\nsubbed-04\np-05\npr-05\nroh-05\npro-05\nmgr-05\nmhi-05\nmpv-05\nrb-05\npw-05\npaa-05\nph-05\npai-05\npa-05\npnp-05\npm-05\npo-05\nef-05\ne-05\ner-05\nefr-05\nlf-05\nimputed-05\nsubbed-05\np-06\npr-06\nroh-06\npro-06\nmgr-06\nmhi-06\nmpv-06\nrb-06\npw-06\npaa-06\nph-06\npai-06\npa-06\npnp-06\npm-06\npo-06\nef-06\ne-06\ner-06\nefr-06\nlf-06\nimputed-06\nsubbed-06\np-07\npr-07\nroh-07\npro-07\nmgr-07\nmhi-07\nmpv-07\nrb-07\npw-07\npaa-07\nph-07\npai-07\npa-07\npnp-07\npm-07\npo-07\nef-07\ne-07\ner-07\nefr-07\nlf-07\nimputed-07\nsubbed-07\np-08\npr-08\nroh-08\npro-08\nmgr-08\nmhi-08\nmpv-08\nrb-08\npw-08\npaa-08\nph-08\npai-08\npa-08\npnp-08\npm-08\npo-08\nef-08\ne-08\ner-08\nefr-08\nlf-08\nimputed-08\nsubbed-08\np-09\npr-09\nroh-09\npro-09\nmgr-09\nmhi-09\nmpv-09\nrb-09\npw-09\npaa-09\nph-09\npai-09\npa-09\npnp-09\npm-09\npo-09\nef-09\ne-09\ner-09\nefr-09\nlf-09\nimputed-09\nsubbed-09\np-10\npr-10\nroh-10\npro-10\nmgr-10\nmhi-10\nmpv-10\nrb-10\npw-10\npaa-10\nph-10\npai-10\npa-10\npnp-10\npm-10\npo-10\nef-10\ne-10\ner-10\nefr-10\nlf-10\nimputed-10\nsubbed-10\np-11\npr-11\nroh-11\npro-11\nmgr-11\nmhi-11\nmpv-11\nrb-11\npw-11\npaa-11\nph-11\npai-11\npa-11\npnp-11\npm-11\npo-11\nef-11\ne-11\ner-11\nefr-11\nlf-11\nimputed-11\nsubbed-11\np-12\npr-12\nroh-12\npro-12\nmgr-12\nmhi-12\nmpv-12\nrb-12\npw-12\npaa-12\nph-12\npai-12\npa-12\npnp-12\npm-12\npo-12\nef-12\ne-12\ner-12\nefr-12\nlf-12\nimputed-12\nsubbed-12\np-13\npr-13\nroh-13\npro-13\nmgr-13\nmhi-13\nmpv-13\nrb-13\npw-13\npaa-13\nph-13\npai-13\npa-13\npnp-13\npm-13\npo-13\nef-13\ne-13\ner-13\nefr-13\nlf-13\nimputed-13\nsubbed-13\np-14\npr-14\nroh-14\npro-14\nmgr-14\nmhi-14\nmpv-14\nrb-14\npw-14\npaa-14\nph-14\npai-14\npa-14\npnp-14\npm-14\npo-14\nef-14\ne-14\ner-14\nefr-14\nlf-14\nimputed-14\nsubbed-14\np-15\npr-15\nroh-15\npro-15\nmgr-15\nmhi-15\nmpv-15\nrb-15\npw-15\npaa-15\nph-15\npai-15\npa-15\npnp-15\npm-15\npo-15\nef-15\ne-15\ner-15\nefr-15\nlf-15\nimputed-15\nsubbed-15\np-16\npr-16\nroh-16\npro-16\nmgr-16\nmhi-16\nmpv-16\nrb-16\npw-16\npaa-16\nph-16\npai-16\npa-16\npnp-16\npm-16\npo-16\nef-16\ne-16\ner-16\nefr-16\nlf-16\nimputed-16\nsubbed-16\ngeometry\n\n\n\n\n435\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\n25.0\n21.0\n1.51\n1.80\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.00\n1.40\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.70\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.40\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.70\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.00\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.00\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.00\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.00\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.00\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.00\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.10\n0.17\n8.79\n0.0\n2.49\n0.00\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\n\n\n436\n42101000200\n-75.1631\n39.9529\n-75.1511\n39.9578\n2\nPhiladelphia County, Pennsylvania\n1362.00\n56.42\n374.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1362.00\n56.42\n415.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1362.00\n56.42\n455.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\n4.0\n4.0\n0.88\n0.88\n1.0\n0.0\n0.0\n1362.00\n56.42\n496.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\n3.0\n3.0\n0.60\n0.60\n1.0\n0.0\n0.0\n1362.00\n56.42\n537.0\n81.48\n421.0\n8349.0\n55600.0\n31.2\n11.16\n5.21\n1.69\n0.07\n79.59\n0.07\n2.20\n0.00\n6.0\n6.0\n1.12\n1.12\n1.0\n0.0\n0.0\n1633.00\n3.45\n578.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n1.0\n0.0\n0.00\n0.17\n1.0\n0.0\n0.0\n1633.00\n3.45\n618.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n6.0\n6.0\n0.97\n0.97\n1.0\n0.0\n0.0\n1633.00\n3.45\n659.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n9.0\n7.0\n1.06\n1.37\n0.0\n0.0\n1.0\n1633.00\n3.45\n700.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n11.0\n7.0\n1.00\n1.57\n0.0\n0.0\n1.0\n1633.00\n3.45\n740.0\n56.04\n675.0\n42083.0\n232800.0\n24.7\n19.29\n2.82\n1.22\n0.0\n76.00\n0.0\n0.67\n0.00\n6.0\n5.0\n0.68\n0.81\n0.0\n0.0\n1.0\n2937.0\n5.07\n781.0\n68.21\n905.0\n49928.0\n261100.0\n26.4\n22.64\n9.67\n2.69\n0.10\n63.16\n0.03\n1.40\n0.31\n6.0\n1.0\n0.13\n0.77\n0.0\n0.0\n1.0\n2331.0\n15.78\n792.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n9.0\n6.0\n0.76\n1.14\n0.0\n0.0\n1.0\n2331.0\n15.78\n802.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n8.0\n3.0\n0.37\n1.00\n0.0\n0.0\n1.0\n2331.0\n15.78\n813.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n14.0\n10.0\n1.23\n1.72\n0.0\n0.0\n1.0\n2331.0\n15.78\n824.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n5.0\n3.0\n0.36\n0.61\n0.0\n0.0\n1.0\n2331.0\n15.78\n834.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n10.0\n9.0\n1.08\n1.20\n0.0\n0.0\n1.0\n2331.0\n15.78\n845.0\n60.27\n1263.0\n59891.0\n265400.0\n28.1\n38.91\n6.05\n1.54\n0.47\n50.75\n0.0\n2.27\n0.00\n11.0\n8.0\n0.95\n1.30\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\n\n\n437\n42101000300\n-75.1798\n39.9544\n-75.1623\n39.9599\n3\nPhiladelphia County, Pennsylvania\n2570.00\n12.16\n861.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2570.00\n12.16\n915.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2570.00\n12.16\n969.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\n14.0\n12.0\n1.24\n1.44\n1.0\n0.0\n0.0\n2570.00\n12.16\n1023.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\n21.0\n17.0\n1.66\n2.05\n1.0\n0.0\n0.0\n2570.00\n12.16\n1077.0\n69.49\n688.0\n40625.0\n233900.0\n29.0\n70.86\n14.67\n3.81\n0.27\n7.00\n0.08\n3.04\n0.27\n23.0\n23.0\n2.13\n2.13\n1.0\n0.0\n0.0\n4497.00\n1.63\n1132.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n12.0\n10.0\n0.88\n1.06\n1.0\n0.0\n0.0\n4497.00\n1.63\n1186.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n19.0\n16.0\n1.35\n1.60\n1.0\n0.0\n0.0\n4497.00\n1.63\n1240.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n21.0\n7.0\n0.56\n1.69\n0.0\n0.0\n1.0\n4497.00\n1.63\n1294.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n25.0\n11.0\n0.85\n1.93\n0.0\n0.0\n1.0\n4497.00\n1.63\n1348.0\n65.66\n1184.0\n59189.0\n438500.0\n24.8\n67.44\n10.52\n5.69\n0.2\n14.14\n0.0\n1.29\n0.71\n27.0\n12.0\n0.89\n2.00\n0.0\n0.0\n1.0\n3169.0\n7.20\n1402.0\n75.58\n1827.0\n71250.0\n451800.0\n28.0\n72.26\n10.22\n4.26\n0.03\n10.35\n0.03\n2.52\n0.32\n24.0\n13.0\n0.93\n1.71\n0.0\n0.0\n1.0\n3405.0\n4.17\n1489.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n21.0\n8.0\n0.54\n1.41\n0.0\n0.0\n1.0\n3405.0\n4.17\n1575.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n27.0\n12.0\n0.76\n1.71\n0.0\n0.0\n1.0\n3405.0\n4.17\n1662.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n31.0\n10.0\n0.60\n1.87\n0.0\n0.0\n1.0\n3405.0\n4.17\n1749.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n27.0\n14.0\n0.80\n1.54\n0.0\n0.0\n1.0\n3405.0\n4.17\n1835.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n18.0\n5.0\n0.27\n0.98\n0.0\n0.0\n1.0\n3405.0\n4.17\n1922.0\n70.47\n1938.0\n81950.0\n469900.0\n26.2\n72.19\n5.26\n8.75\n0.00\n12.04\n0.0\n1.76\n0.00\n26.0\n14.0\n0.73\n1.35\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\n\n\n438\n42101000801\n-75.1833\n39.9486\n-75.1773\n39.9515\n8.01\nPhiladelphia County, Pennsylvania\n1478.00\n14.40\n810.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1478.00\n14.40\n801.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n1478.00\n14.40\n793.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n7.0\n5.0\n0.63\n0.88\n1.0\n0.0\n0.0\n1478.00\n14.40\n784.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n19.0\n13.0\n1.66\n2.42\n1.0\n0.0\n0.0\n1478.00\n14.40\n775.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n17.0\n14.0\n1.81\n2.19\n1.0\n0.0\n0.0\n1344.37\n11.10\n767.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n10.0\n6.0\n0.78\n1.30\n1.0\n0.0\n0.0\n1344.37\n11.10\n758.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n12.0\n7.0\n0.92\n1.58\n1.0\n0.0\n0.0\n1344.37\n11.10\n749.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n12.0\n5.0\n0.67\n1.60\n0.0\n0.0\n1.0\n1344.37\n11.10\n740.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n11.0\n4.0\n0.54\n1.49\n0.0\n0.0\n1.0\n1344.37\n11.10\n732.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n10.0\n2.0\n0.27\n1.37\n0.0\n0.0\n1.0\n1562.0\n2.46\n723.0\n71.09\n2001.0\n83125.0\n459900.0\n25.9\n78.04\n2.94\n5.76\n0.00\n10.82\n0.26\n1.92\n0.26\n14.0\n4.0\n0.55\n1.94\n0.0\n0.0\n1.0\n1692.0\n3.25\n734.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n13.0\n7.0\n0.95\n1.77\n0.0\n0.0\n1.0\n1692.0\n3.25\n746.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n7.0\n0.0\n0.00\n0.94\n0.0\n0.0\n1.0\n1692.0\n3.25\n757.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n15.0\n3.0\n0.40\n1.98\n0.0\n0.0\n1.0\n1692.0\n3.25\n768.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n10.0\n4.0\n0.52\n1.30\n0.0\n0.0\n1.0\n1692.0\n3.25\n780.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n16.0\n8.0\n1.03\n2.05\n0.0\n0.0\n1.0\n1692.0\n3.25\n791.0\n74.87\n2219.0\n81620.0\n656300.0\n24.7\n75.18\n10.64\n4.91\n0.00\n4.08\n0.0\n1.42\n3.78\n13.0\n4.0\n0.51\n1.64\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\n\n\n439\n42101000804\n-75.1712\n39.9470\n-75.1643\n39.9501\n8.04\nPhiladelphia County, Pennsylvania\n3301.00\n14.40\n2058.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n3301.00\n14.40\n2050.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n3301.00\n14.40\n2042.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n22.0\n18.0\n0.88\n1.08\n1.0\n0.0\n0.0\n3301.00\n14.40\n2033.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n31.0\n21.0\n1.03\n1.52\n1.0\n0.0\n0.0\n3301.00\n14.40\n2025.0\n73.65\n933.0\n42346.0\n265200.0\n27.6\n81.67\n2.97\n3.50\n0.04\n10.08\n0.04\n1.26\n0.45\n18.0\n15.0\n0.74\n0.89\n1.0\n0.0\n0.0\n3002.54\n11.10\n2017.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n28.0\n19.0\n0.94\n1.39\n1.0\n0.0\n0.0\n3002.54\n11.10\n2009.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n14.0\n13.0\n0.65\n0.70\n1.0\n0.0\n0.0\n3002.54\n11.10\n2001.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n33.0\n11.0\n0.55\n1.65\n0.0\n0.0\n1.0\n3002.54\n11.10\n1992.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n17.0\n4.0\n0.20\n0.85\n0.0\n0.0\n1.0\n3002.54\n11.10\n1984.0\n66.11\n1393.0\n61590.0\n431800.0\n28.2\n86.58\n1.05\n3.14\n0.0\n8.16\n0.0\n0.88\n0.18\n27.0\n8.0\n0.40\n1.36\n0.0\n0.0\n1.0\n3609.0\n7.69\n1976.0\n76.32\n1562.0\n75357.0\n330200.0\n26.0\n78.55\n2.72\n4.96\n0.03\n11.75\n0.03\n1.72\n0.25\n43.0\n13.0\n0.66\n2.18\n0.0\n0.0\n1.0\n3746.0\n0.00\n2000.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n38.0\n9.0\n0.45\n1.90\n0.0\n0.0\n1.0\n3746.0\n0.00\n2024.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n31.0\n16.0\n0.79\n1.53\n0.0\n0.0\n1.0\n3746.0\n0.00\n2048.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n27.0\n8.0\n0.39\n1.32\n0.0\n0.0\n1.0\n3746.0\n0.00\n2072.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n28.0\n11.0\n0.53\n1.35\n0.0\n0.0\n1.0\n3746.0\n0.00\n2096.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n18.0\n7.0\n0.33\n0.86\n0.0\n0.0\n1.0\n3746.0\n0.00\n2120.0\n75.43\n1816.0\n96250.0\n465500.0\n23.7\n67.43\n4.51\n13.59\n0.35\n13.59\n0.0\n0.19\n0.35\n22.0\n7.0\n0.33\n1.04\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\n\n\n\n\n\n\n\n\n\n1.1.3 Transform from wide to tidy format\nFor this assignment, we are interested in the number of evictions by census tract for various years. Right now, each year has it’s own column, so it will be easiest to transform to a tidy format.\nUse the pd.melt() function to transform the eviction data into tidy format, using the number of evictions from 2003 to 2016.\nThe tidy data frame should have four columns: GEOID, geometry, a column holding the number of evictions, and a column telling you what the name of the original column was for that value.\nHints: - You’ll want to specify the GEOID and geometry columns as the id_vars. This will keep track of the census tract information. - You should specify the names of the columns holding the number of evictions as the value_vars. - You can generate a list of this column names using Python’s f-string formatting: python     value_vars = [f\"e-{x:02d}\" for x in range(3, 17)]\n\n# Generate the list of eviction columns for years 2003 to 2016\nvalue_vars = [f\"e-{x:02d}\" for x in range(3, 17)]\n\n\nphilly_evictions = pd.melt(\n    philly_tracts,\n    id_vars=['GEOID', 'geometry'],  \n    value_vars=value_vars,  \n    var_name='year',  \n    value_name='evictions' \n)\n\nphilly_evictions.head()\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-03\n13.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-03\n21.0\n\n\n\n\n\n\n\n\n\n1.1.4 Plot the total number of evictions per year from 2003 to 2016\nUse hvplot to plot the total number of evictions from 2003 to 2016. You will first need to perform a group by operation and sum up the total number of evictions for all census tracts, and then use hvplot() to make your plot.\nYou can use any type of hvplot chart you’d like to show the trend in number of evictions over time.\n\nimport holoviews as hv\nimport hvplot.pandas\n\n\n\n\n\n\n\n\n\n\n\n# Group by year and sum the evictions for all census tracts\ntotal_evictions_per_year = philly_evictions.groupby('year')['evictions'].sum().reset_index()\n\n\n# Plot the total number of evictions per year using hvplot\neviction_plot = total_evictions_per_year.hvplot(\n    x='year',\n    y='evictions',\n    kind='line',\n    title='Total Number of Evictions in Philadelphia (2003-2016)',\n    xlabel='Year',\n    ylabel='Total Number of Evictions',\n    line_width=2,\n    grid=True,\n    width=600, height=400\n)\n\neviction_plot\n\n\n\n\n\n  \n\n\n\n\n\n\n1.1.5 The number of evictions across Philadelphia\nOur tidy data frame is still a GeoDataFrame with a geometry column, so we can visualize the number of evictions for all census tracts.\nUse hvplot() to generate a choropleth showing the number of evictions for a specified year, with a widget dropdown to select a given year (or variable name, e.g., e-16, e-15, etc).\nHints - You’ll need to use the groupby keyword to tell hvplot to make a series of maps, with a widget to select between them. - You will need to specify dynamic=False as a keyword argument to the hvplot() function. - Be sure to specify a width and height that makes your output map (roughly) square to limit distortions\n\nchoropleth = philly_evictions.hvplot(\n    geo=True,  \n    c='evictions',  \n    frame_width=600,  \n    frame_height=600,  \n    cmap='viridis',  \n    hover_cols=['GEOID'], \n    groupby='year',  # Group by the 'year' column to create an interactive dropdown for different years\n    title='Number of Evictions in Philadelphia Census Tracts by Year',\n    dynamic=False \n)\n\nchoropleth",
    "crumbs": [
      "Analysis",
      "Assignment 3"
    ]
  },
  {
    "objectID": "analysis/assignment-3.html#code-violations-in-philadelphia",
    "href": "analysis/assignment-3.html#code-violations-in-philadelphia",
    "title": "Assignment 3",
    "section": "1.2 Code Violations in Philadelphia",
    "text": "1.2 Code Violations in Philadelphia\nNext, we’ll explore data for code violations from the Licenses and Inspections Department of Philadelphia to look for potential correlations with the number of evictions.\n\n1.2.1 Load data from 2012 to 2016\nL+I violation data for years including 2012 through 2016 (inclusive) is provided in a CSV format in the “data/” folder.\nLoad the data using pandas and convert to a GeoDataFrame.\n\nli_violations = pd.read_csv(\"data/li_violations.csv\")\nli_violations.head()\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\n\n\n\n\n0\n40.050526\n-75.126076\nCLIP VIOLATION NOTICE\n\n\n1\n40.050593\n-75.126578\nLICENSE-CHANGE OF ADDRESS\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\n\n\n3\n39.991994\n-75.128895\nEXT A-CLEAN WEEDS/PLANTS\n\n\n4\n40.023260\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\n\n\n\n\n\n\n\n\n# Use the helper utility function: geopandas.points_from_xy() to create Point objects for each lat and lon combination.\n\nli_violations[\"geometry\"] = gpd.points_from_xy(\n    li_violations[\"lng\"], li_violations[\"lat\"]\n)\nli_violations[\"geometry\"].head()\n\n0    POINT (-75.12608 40.05053)\n1    POINT (-75.12658 40.05059)\n2    POINT (-75.12658 40.05059)\n3    POINT (-75.12889 39.99199)\n4    POINT (-75.16485 40.02326)\nName: geometry, dtype: geometry\n\n\n\n# Create GeoDataFrame\nli_violations_gdf = gpd.GeoDataFrame(\n    li_violations, geometry=\"geometry\", crs=\"EPSG:4326\"\n)\n\n\n\n1.2.2 Trim to specific violation types\nThere are many different types of code violations (running the nunique() function on the violationdescription column will extract all of the unique ones). More information on different types of violations can be found on the City’s website.\nBelow, I’ve selected 15 types of violations that deal with property maintenance and licensing issues. We’ll focus on these violations. The goal is to see if these kinds of violations are correlated spatially with the number of evictions in a given area.\nUse the list of violations given to trim your data set to only include these types.\n\n# View the number of different types of violations\nunique_violation_types = li_violations_gdf['violationdescription'].nunique()\nprint(f\"Number of unique violation types: {unique_violation_types}\")\n\nNumber of unique violation types: 1342\n\n\n\nviolation_types = [\n    \"INT-PLMBG MAINT FIXTURES-RES\",\n    \"INT S-CEILING REPAIR/MAINT SAN\",\n    \"PLUMBING SYSTEMS-GENERAL\",\n    \"CO DETECTOR NEEDED\",\n    \"INTERIOR SURFACES\",\n    \"EXT S-ROOF REPAIR\",\n    \"ELEC-RECEPTABLE DEFECTIVE-RES\",\n    \"INT S-FLOOR REPAIR\",\n    \"DRAINAGE-MAIN DRAIN REPAIR-RES\",\n    \"DRAINAGE-DOWNSPOUT REPR/REPLC\",\n    \"LIGHT FIXTURE DEFECTIVE-RES\",\n    \"LICENSE-RES SFD/2FD\",\n    \"ELECTRICAL -HAZARD\",\n    \"VACANT PROPERTIES-GENERAL\",\n    \"INT-PLMBG FIXTURES-RES\",\n]\n\n\n# Filter the GeoDataFrame to only include rows with the specified violation types\nfiltered_violations_gdf = li_violations_gdf[li_violations_gdf['violationdescription'].isin(violation_types)]\n\n# Check the filtered GeoDataFrame\nfiltered_violations_gdf.head()\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\n\n\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n\n\n25\n40.022406\n-75.121872\nEXT S-ROOF REPAIR\nPOINT (-75.12187 40.02241)\n\n\n30\n40.023237\n-75.121726\nCO DETECTOR NEEDED\nPOINT (-75.12173 40.02324)\n\n\n31\n40.023397\n-75.122241\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-75.12224 40.02340)\n\n\n34\n40.023773\n-75.121603\nINT S-FLOOR REPAIR\nPOINT (-75.12160 40.02377)\n\n\n\n\n\n\n\n\n\n1.2.3 Make a hex bin map\nThe code violation data is point data. We can get a quick look at the geographic distribution using matplotlib and the hexbin() function. Make a hex bin map of the code violations and overlay the census tract outlines.\nHints: - The eviction data from part 1 was by census tract, so the census tract geometries are available as part of that GeoDataFrame. You can use it to overlay the census tracts on your hex bin map. - Make sure you convert your GeoDataFrame to a CRS that’s better for visualization than plain old 4326.\n\n# Convert GeoDataFrame to a projection suitable for visualization\nviolations_3857 = filtered_violations_gdf.to_crs(epsg=3857)\n\n\n# Create the axes\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Extract out the x/y coordindates of the Point objects\nxcoords = violations_3857.geometry.x\nycoords = violations_3857.geometry.y\n\n# Plot a hexbin chart\nhex_violations = ax.hexbin(xcoords, ycoords, gridsize=50)\n\n# Add the geometry boundaries\nphilly_tracts.to_crs(violations_3857.crs).plot(\n    ax=ax, facecolor=\"none\", edgecolor=\"white\", linewidth=0.25\n)\n\n# Add a colorbar and format\nfig.colorbar(hex_vals, ax=ax)\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n\n\n\n\n\n\n\n\n\n1.2.4 Spatially join data sets\nTo do a census tract comparison to our eviction data, we need to find which census tract each of the code violations falls into. Use the geopandas.sjoin() function to do just that.\nHints - You can re-use your eviction data frame, but you will only need the geometry column (specifying census tract polygons) and the GEOID column (specifying the name of each census tract). - Make sure both data frames have the same CRS before joining them together!\n\n# Spatially join the filtered violations with the census tracts to identify which tract each violation falls into\nviolations_with_tracts = gpd.sjoin(\n    filtered_violations_gdf,  # The data for violations\n    philly_tracts[['geometry', 'GEOID']].to_crs(filtered_violations_gdf.crs),  # The census tracts (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\n\nviolations_with_tracts.head()\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\nindex_right\nGEOID\n\n\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n3185\n42101027100\n\n\n25\n40.022406\n-75.121872\nEXT S-ROOF REPAIR\nPOINT (-75.12187 40.02241)\n2238\n42101028800\n\n\n30\n40.023237\n-75.121726\nCO DETECTOR NEEDED\nPOINT (-75.12173 40.02324)\n2238\n42101028800\n\n\n31\n40.023397\n-75.122241\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-75.12224 40.02340)\n2238\n42101028800\n\n\n34\n40.023773\n-75.121603\nINT S-FLOOR REPAIR\nPOINT (-75.12160 40.02377)\n2238\n42101028800\n\n\n\n\n\n\n\n\n\n1.2.5 Calculate the number of violations by type per census tract\nNext, we’ll want to find the number of violations (for each kind) per census tract. You should group the data frame by violation type and census tract name.\nThe result of this step should be a data frame with three columns: violationdescription, GEOID, and N, where N is the number of violations of that kind in the specified census tract.\nOptional: to make prettier plots\nSome census tracts won’t have any violations, and they won’t be included when we do the above calculation. However, there is a trick to set the values for those census tracts to be zero. After you calculate the sizes of each violation/census tract group, you can run:\nN = N.unstack(fill_value=0).stack().reset_index(name='N')\nwhere N gives the total size of each of the groups, specified by violation type and census tract name.\nSee this StackOverflow post for more details.\nThis part is optional, but will make the resulting maps a bit prettier.\n\n# Calculate the number of violations by type per census tract\nviolation_counts = violations_with_tracts.groupby(['violationdescription', 'GEOID']).size().reset_index(name='N')\n\ntype(violation_counts)\n\npandas.core.frame.DataFrame\n\n\n\n# Fill in missing census tracts with zero violations\nviolation_counts = violation_counts.set_index(['violationdescription', 'GEOID']).unstack(fill_value=0).stack().reset_index().rename(columns={0: 'N'})\n\nviolation_counts.head()\n\n\n\n\n\n\n\n\nviolationdescription\nGEOID\nN\n\n\n\n\n0\nCO DETECTOR NEEDED\n42101000100\n0\n\n\n1\nCO DETECTOR NEEDED\n42101000200\n0\n\n\n2\nCO DETECTOR NEEDED\n42101000300\n0\n\n\n3\nCO DETECTOR NEEDED\n42101000401\n1\n\n\n4\nCO DETECTOR NEEDED\n42101000402\n1\n\n\n\n\n\n\n\n\n\n1.2.6 Merge with census tracts geometries\nWe now have the number of violations of different types per census tract specified as a regular DataFrame. You can now merge it with the census tract geometries (from your eviction data GeoDataFrame) to create a GeoDataFrame.\nHints - Use pandas.merge() and specify the on keyword to be the column holding census tract names. - Make sure the result of the merge operation is a GeoDataFrame — you will want the GeoDataFrame holding census tract geometries to be the first argument of the pandas.merge() function.\n\n# Merge the violation counts with the census tract geometries to create a GeoDataFrame\nviolations_by_tracts = philly_tracts.merge(violation_counts, on=\"GEOID\")\n\nviolations_by_tracts['N'] = violations_by_tracts['N'].fillna(0)\n\nviolations_by_tracts.head()\n\n\n\n\n\n\n\n\nGEOID\nwest\nsouth\neast\nnorth\nn\npl\np-00\npr-00\nroh-00\npro-00\nmgr-00\nmhi-00\nmpv-00\nrb-00\npw-00\npaa-00\nph-00\npai-00\npa-00\npnp-00\npm-00\npo-00\nef-00\ne-00\ner-00\nefr-00\nlf-00\nimputed-00\nsubbed-00\np-01\npr-01\nroh-01\npro-01\nmgr-01\nmhi-01\nmpv-01\nrb-01\npw-01\npaa-01\nph-01\npai-01\npa-01\npnp-01\npm-01\npo-01\nef-01\ne-01\ner-01\nefr-01\nlf-01\nimputed-01\nsubbed-01\np-02\npr-02\nroh-02\npro-02\nmgr-02\nmhi-02\nmpv-02\nrb-02\npw-02\npaa-02\nph-02\npai-02\npa-02\npnp-02\npm-02\npo-02\nef-02\ne-02\ner-02\nefr-02\nlf-02\nimputed-02\nsubbed-02\np-03\npr-03\nroh-03\npro-03\nmgr-03\nmhi-03\nmpv-03\nrb-03\npw-03\npaa-03\nph-03\npai-03\npa-03\npnp-03\npm-03\npo-03\nef-03\ne-03\ner-03\nefr-03\nlf-03\nimputed-03\nsubbed-03\np-04\npr-04\nroh-04\npro-04\nmgr-04\nmhi-04\nmpv-04\nrb-04\npw-04\npaa-04\nph-04\npai-04\npa-04\npnp-04\npm-04\npo-04\nef-04\ne-04\ner-04\nefr-04\nlf-04\nimputed-04\nsubbed-04\np-05\npr-05\nroh-05\npro-05\nmgr-05\nmhi-05\nmpv-05\nrb-05\npw-05\npaa-05\nph-05\npai-05\npa-05\npnp-05\npm-05\npo-05\nef-05\ne-05\ner-05\nefr-05\nlf-05\nimputed-05\nsubbed-05\np-06\npr-06\nroh-06\npro-06\nmgr-06\nmhi-06\nmpv-06\nrb-06\npw-06\npaa-06\nph-06\npai-06\npa-06\npnp-06\npm-06\npo-06\nef-06\ne-06\ner-06\nefr-06\nlf-06\nimputed-06\nsubbed-06\np-07\npr-07\nroh-07\npro-07\nmgr-07\nmhi-07\nmpv-07\nrb-07\npw-07\npaa-07\nph-07\npai-07\npa-07\npnp-07\npm-07\npo-07\nef-07\ne-07\ner-07\nefr-07\nlf-07\nimputed-07\nsubbed-07\np-08\npr-08\nroh-08\npro-08\nmgr-08\nmhi-08\nmpv-08\nrb-08\npw-08\npaa-08\nph-08\npai-08\npa-08\npnp-08\npm-08\npo-08\nef-08\ne-08\ner-08\nefr-08\nlf-08\nimputed-08\nsubbed-08\np-09\npr-09\nroh-09\npro-09\nmgr-09\nmhi-09\nmpv-09\nrb-09\npw-09\npaa-09\nph-09\npai-09\npa-09\npnp-09\npm-09\npo-09\nef-09\ne-09\ner-09\nefr-09\nlf-09\nimputed-09\nsubbed-09\np-10\npr-10\nroh-10\npro-10\nmgr-10\nmhi-10\nmpv-10\nrb-10\npw-10\npaa-10\nph-10\npai-10\npa-10\npnp-10\npm-10\npo-10\nef-10\ne-10\ner-10\nefr-10\nlf-10\nimputed-10\nsubbed-10\np-11\npr-11\nroh-11\npro-11\nmgr-11\nmhi-11\nmpv-11\nrb-11\npw-11\npaa-11\nph-11\npai-11\npa-11\npnp-11\npm-11\npo-11\nef-11\ne-11\ner-11\nefr-11\nlf-11\nimputed-11\nsubbed-11\np-12\npr-12\nroh-12\npro-12\nmgr-12\nmhi-12\nmpv-12\nrb-12\npw-12\npaa-12\nph-12\npai-12\npa-12\npnp-12\npm-12\npo-12\nef-12\ne-12\ner-12\nefr-12\nlf-12\nimputed-12\nsubbed-12\np-13\npr-13\nroh-13\npro-13\nmgr-13\nmhi-13\nmpv-13\nrb-13\npw-13\npaa-13\nph-13\npai-13\npa-13\npnp-13\npm-13\npo-13\nef-13\ne-13\ner-13\nefr-13\nlf-13\nimputed-13\nsubbed-13\np-14\npr-14\nroh-14\npro-14\nmgr-14\nmhi-14\nmpv-14\nrb-14\npw-14\npaa-14\nph-14\npai-14\npa-14\npnp-14\npm-14\npo-14\nef-14\ne-14\ner-14\nefr-14\nlf-14\nimputed-14\nsubbed-14\np-15\npr-15\nroh-15\npro-15\nmgr-15\nmhi-15\nmpv-15\nrb-15\npw-15\npaa-15\nph-15\npai-15\npa-15\npnp-15\npm-15\npo-15\nef-15\ne-15\ner-15\nefr-15\nlf-15\nimputed-15\nsubbed-15\np-16\npr-16\nroh-16\npro-16\nmgr-16\nmhi-16\nmpv-16\nrb-16\npw-16\npaa-16\nph-16\npai-16\npa-16\npnp-16\npm-16\npo-16\nef-16\ne-16\ner-16\nefr-16\nlf-16\nimputed-16\nsubbed-16\ngeometry\nviolationdescription\nN\n\n\n\n\n0\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n21.0\n1.51\n1.8\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.7\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.7\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.0\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.0\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.0\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.0\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.0\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.0\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\nCO DETECTOR NEEDED\n0\n\n\n1\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n21.0\n1.51\n1.8\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.7\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.7\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.0\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.0\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.0\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.0\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.0\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.0\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\nDRAINAGE-DOWNSPOUT REPR/REPLC\n6\n\n\n2\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n21.0\n1.51\n1.8\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.7\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.7\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.0\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.0\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.0\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.0\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.0\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.0\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\nDRAINAGE-MAIN DRAIN REPAIR-RES\n0\n\n\n3\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n21.0\n1.51\n1.8\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.7\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.7\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.0\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.0\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.0\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.0\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.0\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.0\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\nELEC-RECEPTABLE DEFECTIVE-RES\n0\n\n\n4\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n21.0\n1.51\n1.8\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.7\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.7\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.0\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.0\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.0\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.0\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.0\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.0\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\nELECTRICAL -HAZARD\n1\n\n\n\n\n\n\n\n\n\n1.2.7 Interactive choropleths for each violation type\nNow, we can use hvplot() to create an interactive choropleth for each violation type and add a widget to specify different violation types.\nHints - You’ll need to use the groupby keyword to tell hvplot to make a series of maps, with a widget to select different violation types. - You will need to specify dynamic=False as a keyword argument to the hvplot() function. - Be sure to specify a width and height that makes your output map (roughly) square to limit distortions\n\n# Create an interactive choropleth for each violation type using hvplot\nchoropleth_2 = violations_by_tracts.hvplot.polygons(\n    geo=True,\n    tiles='CartoLight',\n    color='N',\n    groupby='violationdescription',\n    dynamic=False,\n    frame_width=600,\n    frame_height=600,\n    cmap='Viridis',\n    hover_cols=['GEOID', 'N']\n)\n\n# Display the interactive choropleth\nchoropleth_2",
    "crumbs": [
      "Analysis",
      "Assignment 3"
    ]
  },
  {
    "objectID": "analysis/assignment-3.html#a-side-by-side-comparison",
    "href": "analysis/assignment-3.html#a-side-by-side-comparison",
    "title": "Assignment 3",
    "section": "1.3. A side-by-side comparison",
    "text": "1.3. A side-by-side comparison\nFrom the interactive maps of evictions and violations, you should notice a lot of spatial overlap.\nAs a final step, we’ll make a side-by-side comparison to better show the spatial correlations. This will involve a few steps:\n\nTrim the evictions data frame plotted in section 1.1.5 to only include evictions from 2016.\nTrim the L+I violations data frame plotted in section 1.2.7 to only include a single violation type (pick whichever one you want!).\nUse hvplot() to make two interactive choropleth maps, one for the data from step 1. and one for the data in step 2.\nShow these two plots side by side (one row and 2 columns) using the syntax for combining charts.\n\nNote: since we selected a single year and violation type, you won’t need to use the groupby= keyword here.\n\n# Trim the evictions data to only include evictions from 2016\nevictions_2016 = philly_evictions[philly_evictions['year'] == 'e-16'][['GEOID', 'geometry', 'evictions']].copy()\n\n\n# Trim the L+I violations data to only include a single violation type\nselected_violation = violations_by_tracts[violations_by_tracts['violationdescription'] == 'CO DETECTOR NEEDED']\n\n\n# Create an interactive choropleth for evictions in 2016 using hvplot\nchoropleth_evictions = evictions_2016.hvplot.polygons(\n    geo=True,\n    tiles='CartoLight',\n    color='evictions_2016',\n    frame_width=600,\n    frame_height=600,\n    cmap='Viridis',\n    hover_cols=['GEOID']\n)\n\n# Create an interactive choropleth for the selected violation type using hvplot\nchoropleth_violation = selected_violation.hvplot.polygons(\n    geo=True,\n    tiles='CartoLight',\n    color='N',\n    frame_width=600,\n    frame_height=600,\n    cmap='Viridis',\n    hover_cols=['GEOID', 'N']\n)\n\n# Display the two choropleths side by side\ncomparison = (choropleth_evictions + choropleth_violation).cols(2)\n\n# Display the side-by-side comparison\ncomparison",
    "crumbs": [
      "Analysis",
      "Assignment 3"
    ]
  },
  {
    "objectID": "analysis/assignment-3.html#extra-credit",
    "href": "analysis/assignment-3.html#extra-credit",
    "title": "Assignment 3",
    "section": "1.4. Extra Credit",
    "text": "1.4. Extra Credit\nIdentify the 20 most common types of violations within the time period of 2012 to 2016 and create a set of interactive choropleths similar to what was done in section 1.2.7.\nUse this set of maps to identify 3 types of violations that don’t seem to have much spatial overlap with the number of evictions in the City.\n\n# Identify the 20 most common types of violations from 2012 to 2016\nmost_common_violations = li_violations['violationdescription'].value_counts().head(20).index.tolist()\n\n# Filter the GeoDataFrame to only include rows with the 20 most common violation types\nviolations_gdf_20 = li_violations_gdf[li_violations_gdf['violationdescription'].isin(most_common_violations)]\n\nviolations_gdf_20.head()\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\n\n\n\n\n0\n40.050526\n-75.126076\nCLIP VIOLATION NOTICE\nPOINT (-75.12608 40.05053)\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n\n\n3\n39.991994\n-75.128895\nEXT A-CLEAN WEEDS/PLANTS\nPOINT (-75.12889 39.99199)\n\n\n4\n40.023260\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nPOINT (-75.16485 40.02326)\n\n\n5\n40.023260\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nPOINT (-75.16485 40.02326)\n\n\n\n\n\n\n\n\n# Spatially join the filtered violations with the census tracts to identify which tract each violation falls into\nviolations_with_tracts_20 = gpd.sjoin(\n    violations_gdf_20,  # The point data for violations\n    philly_tracts[['geometry', 'GEOID']].to_crs(filtered_violations_gdf.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\n\nviolations_with_tracts_20.head()\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\nindex_right\nGEOID\n\n\n\n\n0\n40.050526\n-75.126076\nCLIP VIOLATION NOTICE\nPOINT (-75.12608 40.05053)\n3185\n42101027100\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n3185\n42101027100\n\n\n3\n39.991994\n-75.128895\nEXT A-CLEAN WEEDS/PLANTS\nPOINT (-75.12889 39.99199)\n525\n42101017601\n\n\n4\n40.023260\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nPOINT (-75.16485 40.02326)\n3102\n42101024400\n\n\n5\n40.023260\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nPOINT (-75.16485 40.02326)\n3102\n42101024400\n\n\n\n\n\n\n\n\n# Calculate the number of violations by type per census tract\nviolations_counts_20 = violations_with_tracts_20.groupby(['violationdescription', 'GEOID']).size().reset_index(name='N')\n\n# Fill in missing census tracts with zero violations\nviolations_counts_20 = violations_counts_20.set_index(['violationdescription', 'GEOID']).unstack(fill_value=0).stack().reset_index().rename(columns={0: 'N'})\n\nviolations_counts_20.head()\n\n\n\n\n\n\n\n\nviolationdescription\nGEOID\nN\n\n\n\n\n0\nANNUAL CERT FIRE ALARM\n42101000100\n55\n\n\n1\nANNUAL CERT FIRE ALARM\n42101000200\n38\n\n\n2\nANNUAL CERT FIRE ALARM\n42101000300\n27\n\n\n3\nANNUAL CERT FIRE ALARM\n42101000401\n12\n\n\n4\nANNUAL CERT FIRE ALARM\n42101000402\n33\n\n\n\n\n\n\n\n\n# Merge the violation counts with the census tract geometries to create a GeoDataFrame\nviolations_by_tracts_20 = philly_tracts.merge(violations_counts_20, on=\"GEOID\")\n\nviolations_by_tracts_20.head()\n\n\n\n\n\n\n\n\nGEOID\nwest\nsouth\neast\nnorth\nn\npl\np-00\npr-00\nroh-00\npro-00\nmgr-00\nmhi-00\nmpv-00\nrb-00\npw-00\npaa-00\nph-00\npai-00\npa-00\npnp-00\npm-00\npo-00\nef-00\ne-00\ner-00\nefr-00\nlf-00\nimputed-00\nsubbed-00\np-01\npr-01\nroh-01\npro-01\nmgr-01\nmhi-01\nmpv-01\nrb-01\npw-01\npaa-01\nph-01\npai-01\npa-01\npnp-01\npm-01\npo-01\nef-01\ne-01\ner-01\nefr-01\nlf-01\nimputed-01\nsubbed-01\np-02\npr-02\nroh-02\npro-02\nmgr-02\nmhi-02\nmpv-02\nrb-02\npw-02\npaa-02\nph-02\npai-02\npa-02\npnp-02\npm-02\npo-02\nef-02\ne-02\ner-02\nefr-02\nlf-02\nimputed-02\nsubbed-02\np-03\npr-03\nroh-03\npro-03\nmgr-03\nmhi-03\nmpv-03\nrb-03\npw-03\npaa-03\nph-03\npai-03\npa-03\npnp-03\npm-03\npo-03\nef-03\ne-03\ner-03\nefr-03\nlf-03\nimputed-03\nsubbed-03\np-04\npr-04\nroh-04\npro-04\nmgr-04\nmhi-04\nmpv-04\nrb-04\npw-04\npaa-04\nph-04\npai-04\npa-04\npnp-04\npm-04\npo-04\nef-04\ne-04\ner-04\nefr-04\nlf-04\nimputed-04\nsubbed-04\np-05\npr-05\nroh-05\npro-05\nmgr-05\nmhi-05\nmpv-05\nrb-05\npw-05\npaa-05\nph-05\npai-05\npa-05\npnp-05\npm-05\npo-05\nef-05\ne-05\ner-05\nefr-05\nlf-05\nimputed-05\nsubbed-05\np-06\npr-06\nroh-06\npro-06\nmgr-06\nmhi-06\nmpv-06\nrb-06\npw-06\npaa-06\nph-06\npai-06\npa-06\npnp-06\npm-06\npo-06\nef-06\ne-06\ner-06\nefr-06\nlf-06\nimputed-06\nsubbed-06\np-07\npr-07\nroh-07\npro-07\nmgr-07\nmhi-07\nmpv-07\nrb-07\npw-07\npaa-07\nph-07\npai-07\npa-07\npnp-07\npm-07\npo-07\nef-07\ne-07\ner-07\nefr-07\nlf-07\nimputed-07\nsubbed-07\np-08\npr-08\nroh-08\npro-08\nmgr-08\nmhi-08\nmpv-08\nrb-08\npw-08\npaa-08\nph-08\npai-08\npa-08\npnp-08\npm-08\npo-08\nef-08\ne-08\ner-08\nefr-08\nlf-08\nimputed-08\nsubbed-08\np-09\npr-09\nroh-09\npro-09\nmgr-09\nmhi-09\nmpv-09\nrb-09\npw-09\npaa-09\nph-09\npai-09\npa-09\npnp-09\npm-09\npo-09\nef-09\ne-09\ner-09\nefr-09\nlf-09\nimputed-09\nsubbed-09\np-10\npr-10\nroh-10\npro-10\nmgr-10\nmhi-10\nmpv-10\nrb-10\npw-10\npaa-10\nph-10\npai-10\npa-10\npnp-10\npm-10\npo-10\nef-10\ne-10\ner-10\nefr-10\nlf-10\nimputed-10\nsubbed-10\np-11\npr-11\nroh-11\npro-11\nmgr-11\nmhi-11\nmpv-11\nrb-11\npw-11\npaa-11\nph-11\npai-11\npa-11\npnp-11\npm-11\npo-11\nef-11\ne-11\ner-11\nefr-11\nlf-11\nimputed-11\nsubbed-11\np-12\npr-12\nroh-12\npro-12\nmgr-12\nmhi-12\nmpv-12\nrb-12\npw-12\npaa-12\nph-12\npai-12\npa-12\npnp-12\npm-12\npo-12\nef-12\ne-12\ner-12\nefr-12\nlf-12\nimputed-12\nsubbed-12\np-13\npr-13\nroh-13\npro-13\nmgr-13\nmhi-13\nmpv-13\nrb-13\npw-13\npaa-13\nph-13\npai-13\npa-13\npnp-13\npm-13\npo-13\nef-13\ne-13\ner-13\nefr-13\nlf-13\nimputed-13\nsubbed-13\np-14\npr-14\nroh-14\npro-14\nmgr-14\nmhi-14\nmpv-14\nrb-14\npw-14\npaa-14\nph-14\npai-14\npa-14\npnp-14\npm-14\npo-14\nef-14\ne-14\ner-14\nefr-14\nlf-14\nimputed-14\nsubbed-14\np-15\npr-15\nroh-15\npro-15\nmgr-15\nmhi-15\nmpv-15\nrb-15\npw-15\npaa-15\nph-15\npai-15\npa-15\npnp-15\npm-15\npo-15\nef-15\ne-15\ner-15\nefr-15\nlf-15\nimputed-15\nsubbed-15\np-16\npr-16\nroh-16\npro-16\nmgr-16\nmhi-16\nmpv-16\nrb-16\npw-16\npaa-16\nph-16\npai-16\npa-16\npnp-16\npm-16\npo-16\nef-16\ne-16\ner-16\nefr-16\nlf-16\nimputed-16\nsubbed-16\ngeometry\nviolationdescription\nN\n\n\n\n\n0\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n21.0\n1.51\n1.8\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.7\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.7\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.0\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.0\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.0\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.0\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.0\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.0\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\nANNUAL CERT FIRE ALARM\n55\n\n\n1\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n21.0\n1.51\n1.8\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.7\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.7\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.0\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.0\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.0\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.0\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.0\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.0\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\nCLIP VIOLATION NOTICE\n5\n\n\n2\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n21.0\n1.51\n1.8\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.7\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.7\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.0\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.0\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.0\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.0\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.0\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.0\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\nCO DETECTOR NEEDED\n0\n\n\n3\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n21.0\n1.51\n1.8\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.7\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.7\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.0\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.0\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.0\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.0\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.0\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.0\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\nEXT A-CLEAN RUBBISH/GARBAGE\n4\n\n\n4\n42101000100\n-75.1523\n39.9481\n-75.1415\n39.9569\n1\nPhiladelphia County, Pennsylvania\n2646.71\n9.26\n1347.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1360.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n0.0\n2646.71\n9.26\n1374.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n21.0\n19.0\n1.38\n1.53\n1.0\n0.0\n0.0\n2646.71\n9.26\n1388.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n21.0\n1.51\n1.8\n1.0\n0.0\n0.0\n2646.71\n9.26\n1401.0\n77.12\n959.0\n48886.0\n189700.0\n24.5\n78.45\n12.42\n3.47\n0.23\n3.92\n0.0\n1.4\n0.11\n25.0\n24.0\n1.71\n1.78\n1.0\n0.0\n0.0\n3310.88\n12.11\n1415.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n18.0\n15.0\n1.06\n1.27\n1.0\n0.0\n0.0\n3310.88\n12.11\n1428.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n13.0\n10.0\n0.7\n0.91\n1.0\n0.0\n0.0\n3310.88\n12.11\n1442.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n53.0\n20.0\n1.39\n3.68\n0.0\n0.0\n1.0\n3310.88\n12.11\n1456.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n30.0\n17.0\n1.17\n2.06\n0.0\n0.0\n1.0\n3310.88\n12.11\n1469.0\n57.97\n1357.0\n73272.0\n332500.0\n25.6\n83.98\n7.24\n4.4\n0.0\n3.08\n0.0\n0.45\n0.84\n25.0\n11.0\n0.75\n1.7\n0.0\n0.0\n1.0\n3478.0\n3.13\n1483.0\n64.45\n1491.0\n75505.0\n340800.0\n27.5\n83.09\n5.95\n3.62\n0.14\n4.97\n0.06\n2.01\n0.14\n24.0\n18.0\n1.21\n1.62\n0.0\n0.0\n1.0\n3608.0\n0.0\n1524.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n23.0\n11.0\n0.72\n1.51\n0.0\n0.0\n1.0\n3608.0\n0.0\n1565.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n22.0\n7.0\n0.45\n1.41\n0.0\n0.0\n1.0\n3608.0\n0.0\n1606.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n12.0\n0.75\n1.56\n0.0\n0.0\n1.0\n3608.0\n0.0\n1646.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n26.0\n12.0\n0.73\n1.58\n0.0\n0.0\n1.0\n3608.0\n0.0\n1687.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n31.0\n12.0\n0.71\n1.84\n0.0\n0.0\n1.0\n3608.0\n0.0\n1728.0\n71.19\n1545.0\n92207.0\n351600.0\n24.3\n73.45\n10.01\n5.1\n0.17\n8.79\n0.0\n2.49\n0.0\n25.0\n16.0\n0.93\n1.45\n0.0\n0.0\n1.0\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\nEXT A-CLEAN WEEDS/PLANTS\n0\n\n\n\n\n\n\n\n\n# Create interactive choropleths for the 20 most common violations\nchoropleth_violations_20 = violations_by_tracts_20.hvplot.polygons(\n    geo=True,\n    tiles='CartoLight',\n    color='N',\n    groupby='violationdescription',\n    dynamic=False,\n    frame_width=600,\n    frame_height=600,\n    cmap='Viridis',\n    hover_cols=['GEOID', 'N']\n)\n\n# Display the interactive choropleth for the 20 most common violations\nchoropleth_violations_20",
    "crumbs": [
      "Analysis",
      "Assignment 3"
    ]
  },
  {
    "objectID": "analysis/assignment-3.html#part-2-exploring-the-ndvi-in-philadelphia",
    "href": "analysis/assignment-3.html#part-2-exploring-the-ndvi-in-philadelphia",
    "title": "Assignment 3",
    "section": "Part 2: Exploring the NDVI in Philadelphia",
    "text": "Part 2: Exploring the NDVI in Philadelphia\nIn this part, we’ll explore the NDVI in Philadelphia a bit more. This part will include two parts:\n\nWe’ll compare the median NDVI within the city limits and the immediate suburbs\nWe’ll calculate the NDVI around street trees in the city.",
    "crumbs": [
      "Analysis",
      "Assignment 3"
    ]
  },
  {
    "objectID": "analysis/assignment-3.html#comparing-the-ndvi-in-the-city-and-the-suburbs",
    "href": "analysis/assignment-3.html#comparing-the-ndvi-in-the-city-and-the-suburbs",
    "title": "Assignment 3",
    "section": "2.1 Comparing the NDVI in the city and the suburbs",
    "text": "2.1 Comparing the NDVI in the city and the suburbs\n\n2.1.1 Load Landsat data for Philadelphia\nUse rasterio to load the landsat data for Philadelphia (available in the “data/” folder)\n\nimport rasterio as rio\n\n\nlandsat = rio.open(\"./data/landsat8_philly.tif\")\n\nlandsat\n\n&lt;open DatasetReader name='./data/landsat8_philly.tif' mode='r'&gt;\n\n\n\n# The CRS\nlandsat.crs\n\n# The bounds\nlandsat.bounds\n\n# The number of bands available\nlandsat.count\n\n# The band numbers that are available\nlandsat.indexes\n\n# Number of pixels in the x and y directions\nlandsat.shape\n\n# The 6 parameters that map from pixel to real space\nlandsat.transform\n\n# All of the meta data\nlandsat.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint16',\n 'nodata': None,\n 'width': 923,\n 'height': 999,\n 'count': 10,\n 'crs': CRS.from_epsg(32618),\n 'transform': Affine(30.0, 0.0, 476064.3596176505,\n        0.0, -30.0, 4443066.927074196)}\n\n\n\n\n2.1.2 Separating the city from the suburbs\nCreate two polygon objects, one for the city limits and one for the suburbs. To calculate the suburbs polygon, we will take everything outside the city limits but still within the bounding box.\n\nThe city limits are available in the “data/” folder.\nTo calculate the suburbs polygon, the “envelope” attribute of the city limits geometry will be useful.\nYou can use geopandas’ geometric manipulation functionality to calculate the suburbs polygon from the city limits polygon and the envelope polygon.\n\n\n# Load data and convert to the correct CRS\ncity_limits = gpd.read_file(\"data/City_Limits.geojson\")\ncity_limits = city_limits.to_crs(epsg=landsat.crs.to_epsg())\ncity_limits.head()\n\n\n\n\n\n\n\n\nOBJECTID\nShape__Area\nShape__Length\ngeometry\n\n\n\n\n0\n1\n0.038911\n1.259687\nPOLYGON ((498724.960 4443066.927, 498759.458 4...\n\n\n\n\n\n\n\n\nimport shapefile\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import shape, Point\nfrom shapely.geometry import Polygon\n\n\n# Get the city limits polygon\ncity_polygon = city_limits.geometry.iloc[0]\n\n# Calculate the bounding box for the city limits\nbounding_box = city_polygon.envelope\n\n# Calculate the suburbs polygon as the difference between the bounding box and the city limits\nsuburbs_polygon = bounding_box.difference(city_polygon)\n\n# Create GeoDataFrame for suburbs\nsuburbs_gdf = gpd.GeoDataFrame(\n    {\n        'name': ['Suburbs'],\n        'geometry': [suburbs_polygon]\n    },\n    crs=city_limits.crs\n)\n\nsuburbs_gdf.head()\n\n\n\n\n\n\n\n\nname\ngeometry\n\n\n\n\n0\nSuburbs\nMULTIPOLYGON (((476064.360 4413069.557, 476064...\n\n\n\n\n\n\n\n\n\n2.1.3 Mask and calculate the NDVI for the city and the suburbs\nUsing the two polygons from the last section, use rasterio’s mask functionality to create two masked arrays from the landsat data, one for the city and one for the suburbs.\nFor each masked array, calculate the NDVI.\n\nMask\n\nfrom rasterio.mask import mask\n\n\n# Mask the city area\ncity_masked, city_transform = mask(\n    dataset=landsat,               # The original raster data\n    shapes=[city_polygon],         # city\n    crop=True,                     # remove pixels not within boundary\n    all_touched=True,              # get all pixels that touch the boudnary\n    filled=False                   # do not fill cropped pixels with a default value\n)\n\n# Mask the suburb area\nsuburbs_masked, suburbs_transform = mask(\n    dataset=landsat,               \n    shapes=[suburbs_polygon],      # suburb\n    crop=True,                     \n    all_touched=True,              \n    filled=False                   \n)\n\n\n\nCalculate NDVI for city and suburbs\nFormula: NDVI = (NIR - Red) / (NIR + Red)\nNIR is band 5 and Red is band 4, but the indexing here is zero-based, e.g., band 5 is index 4\n\ncity_red = city_masked[3]  \ncity_nir = city_masked[4]  \n\nsuburbs_red = suburbs_masked[3]  \nsuburbs_nir = suburbs_masked[4]  \n\n\ndef calculate_NDVI(city_nir, city_red):\n    \"\"\"\n    Calculate the NDVI from the NIR and red landsat bands\n    \"\"\"\n\n    # Convert to floats\n    city_nir = city_nir.astype(float)\n    city_red = city_red.astype(float)\n\n    # Get valid entries\n    city_check = np.logical_and(city_red.mask == False, city_nir.mask == False)\n\n    # Where the check is True, return the NDVI, else return NaN\n    city_ndvi = np.where(city_check, (city_nir - city_red) / (city_nir + city_red), np.nan)\n    \n    # Return\n    return city_ndvi\n\n\ndef calculate_NDVI(suburbs_nir, suburbs_red):\n    \"\"\"\n    Calculate the NDVI from the NIR and red landsat bands\n    \"\"\"\n\n    # Convert to floats\n    suburbs_nir = suburbs_nir.astype(float)\n    suburbs_red = suburbs_red.astype(float)\n\n    # Get valid entries\n    suburbs_check = np.logical_and(suburbs_red.mask == False, suburbs_nir.mask == False)\n\n    # Where the check is True, return the NDVI, else return NaN\n    suburbs_ndvi = np.where(suburbs_check, (suburbs_nir - suburbs_red) / (suburbs_nir + suburbs_red), np.nan)\n    \n    # Return\n    return suburbs_ndvi\n\n\ncity_NDVI = calculate_NDVI(city_nir, city_red)\nsuburbs_NDVI = calculate_NDVI(suburbs_nir, suburbs_red)\n\nCity\n\n# The extent of the data\nlandsat_extent = [\n    landsat.bounds.left,\n    landsat.bounds.right,\n    landsat.bounds.bottom,\n    landsat.bounds.top,\n]\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot NDVI\ncity_img = ax.imshow(city_NDVI, extent=landsat_extent)\n\n# Format and plot city limits\ncity_limits.plot(ax=ax, edgecolor=\"gray\", facecolor=\"none\", linewidth=2)\nplt.colorbar(city_img)\nax.set_axis_off()\nax.set_title(\"NDVI in Philadelphia\", fontsize=18);\n\n\n\n\n\n\n\n\nSuburbs\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot NDVI\nsuburbs_img = ax.imshow(suburbs_NDVI, extent=landsat_extent)\n\n# Format and plot suburbs\nsuburbs_gdf.plot(ax=ax, edgecolor=\"gray\", facecolor=\"none\", linewidth=2)\nplt.colorbar(suburbs_img)\nax.set_axis_off()\nax.set_title(\"NDVI in Suburbs\", fontsize=18);\n\n\n\n\n\n\n\n\n\n\n\n2.1.4 Calculate the median NDVI within the city and within the suburbs\n\nCalculate the median value from your NDVI arrays for the city and suburbs\nNumpy’s nanmedian function will be useful for ignoring NaN elements\nPrint out the median values. Which has a higher NDVI: the city or suburbs?\n\n\nfrom rasterstats import zonal_stats\n\n\nThe Median NDVI within the City\n\ncity_stats = zonal_stats(\n    city_limits,  # The vector data\n    city_NDVI,  # The array holding the raster data\n    affine=landsat.transform,  # The affine transform for the raster data\n    stats=[\"median\"],  # The stats to compute\n    nodata=np.nan,  # Missing data representation\n)\n\n# Extract median NDVI values from the stats result\nmedian_city = [stats_dict[\"median\"] for stats_dict in city_stats]\n\n# Store the median NDVI values in the original GeoDataFrame\ncity_limits[\"median_NDVI\"] = median_city\n\ncity_limits.head()\n\n\n\n\n\n\n\n\nOBJECTID\nShape__Area\nShape__Length\ngeometry\nmedian_NDVI\n\n\n\n\n0\n1\n0.038911\n1.259687\nPOLYGON ((498724.960 4443066.927, 498759.458 4...\n0.202466\n\n\n\n\n\n\n\n\n\nThe Median NDVI within the Suburbs\n\nsuburbs_stats = zonal_stats(\n    suburbs_gdf,  # The vector data\n    suburbs_NDVI,  # The array holding the raster data\n    affine=landsat.transform,  # The affine transform for the raster data\n    stats=[\"median\"],  # The stats to compute\n    nodata=np.nan,  # Missing data representation\n)\n\n# Extract median NDVI values from the stats result\nmedian_suburbs = [stats_dict[\"median\"] for stats_dict in suburbs_stats]\n\n# Store the median NDVI values in the original GeoDataFrame\nsuburbs_gdf[\"median_NDVI\"] = median_suburbs\n\nsuburbs_gdf.head()\n\n\n\n\n\n\n\n\nname\ngeometry\nmedian_NDVI\n\n\n\n\n0\nSuburbs\nMULTIPOLYGON (((476064.360 4413069.557, 476064...\n0.374849\n\n\n\n\n\n\n\n\n\nCompare\n\ncity_median_ndvi = city_limits['median_NDVI'].iloc[0]\nprint(f\"Median NDVI in the city: {city_median_ndvi:.4f}\")\n\nsuburbs_median_ndvi = suburbs_gdf['median_NDVI'].iloc[0]\nprint(f\"Median NDVI in the Suburbs: {suburbs_median_ndvi:.4f}\")\n\n# Determine which has higher NDVI\nif city_median_ndvi &gt; suburbs_median_ndvi:\n    print(\"The city has a higher median NDVI than the suburbs.\")\nelif city_median_ndvi &lt; suburbs_median_ndvi:\n    print(\"The suburbs have a higher median NDVI than the city.\")\nelse:\n    print(\"The city and the suburbs have the same median NDVI.\")\n\nMedian NDVI in the city: 0.2025\nMedian NDVI in the Suburbs: 0.3748\nThe suburbs have a higher median NDVI than the city.",
    "crumbs": [
      "Analysis",
      "Assignment 3"
    ]
  },
  {
    "objectID": "analysis/assignment-3.html#calculating-the-ndvi-for-philadelphias-street-treets",
    "href": "analysis/assignment-3.html#calculating-the-ndvi-for-philadelphias-street-treets",
    "title": "Assignment 3",
    "section": "2.2 Calculating the NDVI for Philadelphia’s street treets",
    "text": "2.2 Calculating the NDVI for Philadelphia’s street treets\n\n2.2.1 Load the street tree data\nThe data is available in the “data/” folder. It has been downloaded from OpenDataPhilly. It contains the locations of abot 2,500 street trees in Philadelphia.\n\n# Load data and convert to the correct CRS\nstreet_trees = gpd.read_file(\"data/ppr_tree_canopy_points_2015.geojson\")\nstreet_trees = street_trees.to_crs(epsg=landsat.crs.to_epsg())\nstreet_trees.head()\n\n\n\n\n\n\n\n\nobjectid\nfcode\ngeometry\n\n\n\n\n0\n1\n3000\nPOINT (499541.269 4434698.265)\n\n\n1\n2\n3000\nPOINT (488932.471 4424093.158)\n\n\n2\n3\n3000\nPOINT (489039.214 4425985.827)\n\n\n3\n4\n3000\nPOINT (488993.171 4426088.005)\n\n\n4\n5\n3000\nPOINT (488943.113 4424599.478)\n\n\n\n\n\n\n\n\n\n2.2.2 Calculate the NDVI values at the locations of the street trees\n\nUse the rasterstats package to calculate the NDVI values at the locations of the street trees.\nSince these are point geometries, you can calculate either the median or the mean statistic (only one pixel will contain each point).\n\n\nstats_by_trees = zonal_stats(\n    street_trees,  # The vector data\n    city_NDVI,  # The array holding the raster data\n    affine=landsat.transform,  # The affine transform for the raster data\n    stats=[\"median\"],  # The stats to compute\n    nodata=np.nan,  # Missing data representation\n)\n\n# Extract median NDVI values from the stats result\nmedian_stats = [stats_dict[\"median\"] for stats_dict in stats_by_trees]\n\n# Store the median NDVI values in the original GeoDataFrame\nstreet_trees[\"median_NDVI\"] = median_stats\n\nstreet_trees.head()\n\n\n\n\n\n\n\n\nobjectid\nfcode\ngeometry\nmedian_NDVI\n\n\n\n\n0\n1\n3000\nPOINT (499541.269 4434698.265)\n0.235337\n\n\n1\n2\n3000\nPOINT (488932.471 4424093.158)\n0.261535\n\n\n2\n3\n3000\nPOINT (489039.214 4425985.827)\n0.096769\n\n\n3\n4\n3000\nPOINT (488993.171 4426088.005)\n0.076630\n\n\n4\n5\n3000\nPOINT (488943.113 4424599.478)\n0.267952\n\n\n\n\n\n\n\n\n\n2.2.3 Plotting the results\nMake two plots of the results:\n\nA histogram of the NDVI values, using matplotlib’s hist function. Include a vertical line that marks the NDVI = 0 threshold\nA plot of the street tree points, colored by the NDVI value, using geopandas’ plot function. Include the city limits boundary on your plot.\n\nThe figures should be clear and well-styled, with for example, labels for axes, legends, and clear color choices.\n\nA histogram of the NDVI values\n\n# Initialize\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot a histogram\nax.hist(street_trees[\"median_NDVI\"], bins=\"auto\")\nax.axvline(x=0, c=\"k\", lw=2)\n\n# Format\nax.set_xlabel('Median NDVI', fontsize=14)\nax.set_ylabel('Number of Trees', fontsize=14)\nax.set_title('Histogram of Median NDVI at Street Tree Locations', fontsize=16)\n\nText(0.5, 1.0, 'Histogram of Median NDVI at Street Tree Locations')\n\n\n\n\n\n\n\n\n\nAs can be seen from the histogram, the NDVI values of most street tree locations are concentrated in the range of 0 to 0.3, among which the number of trees with NDVI values of 0 to 0.1 is the largest, and the frequency is about 300. This suggests that there is generally less vegetation cover in the area where street trees are located. Only a few street trees had an NDVI value of more than 0.4, indicating that the surrounding vegetation was healthy. In general, the vegetation health status of street trees has a large room for improvement, especially in those areas with low NDVI value, so it may be necessary to take measures to improve the vegetation quality.\n\n\nA plot of the street tree points\n\n# Initialize\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot the city limits\ncity_limits.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=4)\n\n# Plot the median NDVI\nstreet_trees.plot(column=\"median_NDVI\", legend=True, ax=ax, cmap=\"viridis\")\n\n# Format\nax.set_axis_off()\nax.set_title('Street Trees Colored by NDVI Value', fontsize=16)\n\nText(0.5, 1.0, 'Street Trees Colored by NDVI Value')\n\n\n\n\n\n\n\n\n\nAs can be seen from the figure, there are obvious differences in the NDVI values of street trees in different urban regions: the northern and northeastern regions have higher NDVI values and better vegetation cover, while the central and southern regions have lower NDVI values and relatively poor vegetation cover. This suggests that the environment around street trees is more favourable in the north and North-East, while more green improvements are needed in the centre and south to improve vegetation health and coverage.",
    "crumbs": [
      "Analysis",
      "Assignment 3"
    ]
  },
  {
    "objectID": "index.html#uber",
    "href": "index.html#uber",
    "title": "MUSA 550: Final Project",
    "section": "",
    "text": "Uber is a popular ride-hailing platform that connects passengers with drivers through its mobile app. Unlike traditional taxi services, Uber does not own any vehicles. Instead, it operates as a digital marketplace where riders can book trips with independent drivers using their personal vehicles. For drivers, Uber offers a flexible earning opportunity, while for passengers, it provides a convenient and often more affordable transportation option.\nThis project focuses on the exploration and visualization of Uber trips in New York City during 2023. It aims to provide actionable insights into spatial and temporal trip patterns, travel demand fluctuations, and predictions of Uber trips in different regions. These insights can help Uber drivers identify high-demand areas and improve their efficiency. The data used in this project primarily come from NYC Taxi & Limousine Commission, OpenStreetMap, and the United States Census Bureau, providing detailed information about trip origins, destinations, fares, trip distances, durations, and demographics.\nThis project is divided into 3 main parts:\n\nExploratory Analysis of Uber Trips in NYC\nSpatial Analysis of High-Demand Zones\nUber Trips Prediction for Drivers in Manhattan\n\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "index.html#exploring-visualizing-and-predicting-uber-trips-in-nyc-2023",
    "href": "index.html#exploring-visualizing-and-predicting-uber-trips-in-nyc-2023",
    "title": "MUSA 550: Final Project",
    "section": "",
    "text": "Uber is a popular ride-hailing platform that connects passengers with drivers through its mobile app. Unlike traditional taxi services, Uber does not own any vehicles. Instead, it operates as a digital marketplace where riders can book trips with independent drivers using their personal vehicles. For drivers, Uber offers a flexible earning opportunity, while for passengers, it provides a convenient and often more affordable transportation option.\nThis project focuses on the exploration and visualization of Uber trips in New York City during 2023. It aims to provide actionable insights into spatial and temporal trip patterns, travel demand fluctuations, and predictions of Uber trips in different regions. These insights can help Uber drivers identify high-demand areas and improve their efficiency. The data used in this project primarily come from NYC Taxi & Limousine Commission, OpenStreetMap, and the United States Census Bureau, providing detailed information about trip origins, destinations, fares, trip distances, durations, and demographics.\nThis project is divided into 3 main parts:\n\nExploratory Analysis of Uber Trips in NYC\nSpatial Analysis of High-Demand Zones\nUber Trips Prediction for Drivers in Manhattan\n\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "index.html#exploring-visualizing-and-predicting-uber-trips-in-nyc",
    "href": "index.html#exploring-visualizing-and-predicting-uber-trips-in-nyc",
    "title": "MUSA 550: Final Project",
    "section": "",
    "text": "Uber is a popular ride-hailing platform that connects passengers with drivers through its mobile app. Unlike traditional taxi services, Uber does not own any vehicles. Instead, it operates as a digital marketplace where riders can book trips with independent drivers using their personal vehicles. For drivers, Uber offers a flexible earning opportunity, while for passengers, it provides a convenient and often more affordable transportation option.\nThis project focuses on the exploration and visualization of Uber trips in New York City during 2023. It aims to provide actionable insights into spatial and temporal trip patterns, travel demand fluctuations, and predictions of Uber trips in different regions. These insights can help Uber drivers identify high-demand areas and improve their efficiency. The data used in this project primarily come from NYC Taxi & Limousine Commission, OpenStreetMap, and the United States Census Bureau, providing detailed information about trip origins, destinations, fares, trip distances, durations, and demographics.\nThis project is divided into 3 main parts:\n\nExploratory Analysis of Uber Trips in NYC\nSpatial Analysis of High-Demand Zones\nUber Trips Prediction for Drivers in Manhattan",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "ExploratoryAnalysisUberTrips.html",
    "href": "ExploratoryAnalysisUberTrips.html",
    "title": "Exploratory Analysis of Uber Trips in NYC",
    "section": "",
    "text": "Data Preparation\nI began by downloading the 2023 ride-hailing and taxi trip data for New York City from NYC Taxi & Limousine Commission. The dataset included trips from multiple service providers, such as Lyft, Yellow Taxi, and Uber. To focus specifically on Uber trips, I filtered the raw data to isolate records related only to Uber. Given the massive size of the dataset (over hundreds of millions of rows), I stored and processed the data in .parquet format to enhance efficiency. Additionally, I leveraged Dask DataFrame to handle the large-scale data and support subsequent analyses.\nNext, I cleaned and refined the data, removing irrelevant information and retaining only the fields essential for analysis—such as location data (pickup and drop-off coordinates), trip duration, and passenger fares.\nFinally, for spatial analysis, I loaded the taxi_zones.shp file, also obtained from NYC Taxi & Limousine Commission. This shapefile served as a spatial reference, enabling me to link trip data to specific geographic zones within New York City.\n\n\nHello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Eric Delmelle, the instructor for the course.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like",
    "crumbs": [
      "Exploratory Analysis of Uber Trips in NYC"
    ]
  },
  {
    "objectID": "ExploratoryAnalysisUberTrips.html#数据准备",
    "href": "ExploratoryAnalysisUberTrips.html#数据准备",
    "title": "Exploratory Analysis of Uber Trips in NYC",
    "section": "",
    "text": "首先，我加载了2023年纽约市的Uber trips（来自NYC Taxi & Limousine Commission），在此基础上清理了原始数据集。因为下载下来的数据包含所有的网约车和出租车数据（如Lyft，yellow taxi 以及 uber等等），我需要从中筛选出属于Uber的行程数据。由于数据量太大（超过rows），我使用了.parquet来储存和读取数据，并用daskframe来进行后续操作\n然后，我删除了无关信息，只留下来包含位置信息的数据，以及与行程相关的数据（如pick location，trip time，passenger fare等）。\n最后，我还加载了taxi_zones.shp（同样来自NYC Taxi & Limousine Commission）作为进一步分析的空间参考。\n首先，我从NYC Taxi & Limousine Commission 下载了 2023 年纽约市的网约车和出租车行程数据，其中包含多个服务提供商（如 Lyft、Yellow Taxi 和 Uber 等）。为了专注于 Uber 的行程分析，我从原始数据集中筛选出了仅属于 Uber 的数据。由于原始数据量较大（上亿行），我采用 .parquet 格式存储和读取数据，以提高处理效率。同时，我使用 Dask DataFrame 处理大规模数据集，为后续分析提供支持。\n接着，我清理并筛选了数据，删除了无关信息，仅保留与分析相关的字段，包括位置信息（如起点和终点坐标）、行程时间、乘客费用等关键变量。\n最后，为进一步的空间分析，我加载了来自NYC Taxi & Limousine Commission 的 taxi_zones.shp 文件作为空间参考数据，以便将行程数据与纽约市各区域进行关联分析。\n\n\nHello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Eric Delmelle, the instructor for the course.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like",
    "crumbs": [
      "Exploratory Analysis of Uber Trips in NYC"
    ]
  },
  {
    "objectID": "ExploratoryAnalysisUberTrips.html#data-prepara",
    "href": "ExploratoryAnalysisUberTrips.html#data-prepara",
    "title": "Exploratory Analysis of Uber Trips in NYC",
    "section": "",
    "text": "I began by downloading the 2023 ride-hailing and taxi trip data for New York City from NYC Taxi & Limousine Commission. The dataset included trips from multiple service providers, such as Lyft, Yellow Taxi, and Uber. To focus specifically on Uber trips, I filtered the raw data to isolate records related only to Uber. Given the massive size of the dataset (over hundreds of millions of rows), I stored and processed the data in .parquet format to enhance efficiency. Additionally, I leveraged Dask DataFrame to handle the large-scale data and support subsequent analyses.\nNext, I cleaned and refined the data, removing irrelevant information and retaining only the fields essential for analysis—such as location data (pickup and drop-off coordinates), trip duration, and passenger fares.\nFinally, for spatial analysis, I loaded the taxi_zones.shp file, also obtained from NYC Taxi & Limousine Commission. This shapefile served as a spatial reference, enabling me to link trip data to specific geographic zones within New York City.\n\n\nHello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Eric Delmelle, the instructor for the course.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like",
    "crumbs": [
      "Exploratory Analysis of Uber Trips in NYC"
    ]
  },
  {
    "objectID": "ExploratoryAnalysisUberTrips.html#data-preparation",
    "href": "ExploratoryAnalysisUberTrips.html#data-preparation",
    "title": "Exploratory Analysis of Uber Trips in NYC",
    "section": "Data Preparation",
    "text": "Data Preparation\nI began by downloading the 2023 ride-hailing and taxi trip data for New York City from NYC Taxi & Limousine Commission. The dataset included trips from multiple service providers, such as Lyft, Yellow Taxi, and Uber. To focus specifically on Uber trips, I filtered the raw data to isolate records related only to Uber. Given the massive size of the dataset (over hundreds of millions of rows), I stored and processed the data in .parquet format to enhance efficiency. Additionally, I leveraged Dask DataFrame to handle the large-scale data and support subsequent analyses.\nNext, I cleaned and refined the data, removing irrelevant information and retaining only the fields essential for analysis—such as location data (pickup and drop-off coordinates), trip duration, and passenger fares.\nFinally, for spatial analysis, I loaded the taxi_zones.shp file, also obtained from NYC Taxi & Limousine Commission. This shapefile served as a spatial reference, enabling me to link trip data to specific geographic zones within New York City.",
    "crumbs": [
      "Exploratory Analysis of Uber Trips in NYC"
    ]
  },
  {
    "objectID": "ExploratoryAnalysisUberTrips.html#travel-time-period-analysis-hvplotpanel",
    "href": "ExploratoryAnalysisUberTrips.html#travel-time-period-analysis-hvplotpanel",
    "title": "Exploratory Analysis of Uber Trips in NYC",
    "section": "Travel Time Period Analysis (hvplot+panel)",
    "text": "Travel Time Period Analysis (hvplot+panel)\n我按星期几对trips进行了时间序列需求分布可视化。We can observe that the time period distribution of the number of Uber trips is different from WEEKDAY and WEEKEND, e.g. 0:00 on Sundays is the peak time, whereas on Mondays the peak time is around 8:00。Open Interactive Panel App Sample：显示 Monday（星期一）不同时间段的 Uber 行程需求分布 \nSo we further look at the relationship between weekday and hour by using heatmap",
    "crumbs": [
      "Exploratory Analysis of Uber Trips in NYC"
    ]
  },
  {
    "objectID": "ExploratoryAnalysisUberTrips.html#travel-time-period-analysis-panel-holoviews-hvplot",
    "href": "ExploratoryAnalysisUberTrips.html#travel-time-period-analysis-panel-holoviews-hvplot",
    "title": "Exploratory Analysis of Uber Trips in NYC",
    "section": "Travel Time Period Analysis (Panel + HoloViews + hvplot)",
    "text": "Travel Time Period Analysis (Panel + HoloViews + hvplot)\nI first analyzed the distribution of Uber trip durations across different weekdays. From the chart, it can be observed that trip durations during weekdays exhibit greater variability, with notably higher medians on Thursday and Friday. In contrast, trip durations on weekends are more concentrated and shorter, which may reflect that weekday trips are primarily driven by commuting needs, resulting in longer and more dispersed durations, whereas weekend trips are likely dominated by shorter leisure travel, leading to more consistent and shorter durations. \nI visualized the time-series distribution of Uber trip demand based on different weekdays. The results reveal distinct patterns between weekdays and weekends. For instance, Sundays exhibit a peak demand at midnight (0:00), likely reflecting late-night activities. In contrast, Mondays show a peak around 8:00 AM, aligning with typical weekday commuting hours. \nSo I further look at the relationship between weekday, weekend and hour by using heatmap. This heatmap shows the distribution of travel demand by day of the week and hour. Weekday demand is higher overall, especially during the morning and evening peaks (6-9 and 17-20), reflecting the commuting demand profile, while the weekend peaks are mainly concentrated at (18-1), showing the travel patterns of nightlife.",
    "crumbs": [
      "Exploratory Analysis of Uber Trips in NYC"
    ]
  },
  {
    "objectID": "Cluster.html#data-preparation",
    "href": "Cluster.html#data-preparation",
    "title": "Weekday vs. Weekend Travel Patterns",
    "section": "Data Preparation",
    "text": "Data Preparation\nBuilding on the observation in Part 1 that there are significant differences in pick-up time periods between weekdays and weekends, I refined the scope of analysis by focusing on Uber trip data from January 2023. To investigate potential differences in travel patterns, I first filtered the January data from the full 2023 dataset and categorized trips into weekdays and weekends based on the time_type.\nThis classification sets the foundation for clustering analysis, which examines key trip characteristics—including trip miles, fare, trip duration, and cost per mile—to provide deeper insights into how travel behaviors vary between weekdays and weekends.",
    "crumbs": [
      "Weekday vs. Weekend Travel Patterns"
    ]
  },
  {
    "objectID": "Cluster.html#travel-time-period-analysis-panel-holoviews-hvplot",
    "href": "Cluster.html#travel-time-period-analysis-panel-holoviews-hvplot",
    "title": "Weekday vs. Weekend Travel Patterns",
    "section": "Travel Time Period Analysis (Panel + HoloViews + hvplot)",
    "text": "Travel Time Period Analysis (Panel + HoloViews + hvplot)\nI visualized the time-series distribution of Uber trip demand based on different weekdays. The results reveal distinct patterns between weekdays and weekends. For instance, Sundays exhibit a peak demand at midnight (0:00), likely reflecting late-night activities. In contrast, Mondays show a peak around 8:00 AM, aligning with typical weekday commuting hours.Open Interactive Panel App\nSample: The chart illustrates the demand distribution for Monday, highlighting variations in trip volume across different hours of the day.  \nSo I further look at the relationship between weekday, weekend and hour by using heatmap. This heatmap shows the distribution of travel demand by day of the week and hour. Weekday demand is higher overall, especially during the morning and evening peaks (6-9 and 17-20), reflecting the commuting demand profile, while the weekend peaks are mainly concentrated at (18-1), showing the travel patterns of nightlife.",
    "crumbs": [
      "Weekday vs. Weekend Travel Patterns"
    ]
  },
  {
    "objectID": "Cluster.html#clustering-analysis-kmeans",
    "href": "Cluster.html#clustering-analysis-kmeans",
    "title": "Weekday vs. Weekend Travel Patterns",
    "section": "Clustering Analysis (KMeans)",
    "text": "Clustering Analysis (KMeans)\nIn the Uber dataset, I aggregate by pickup_location and time_type then extract relevant statistical features.\nFirst, I aggregate the data by pickup_location and time_type, and select fare, trip_miles, trip_duration_min, and cost_per_mile as key features. I then calculate the average values of these features along with the total count for each zone.\nNext, a K-means clustering analysis is performed, dividing the data into 5 distinct clusters to uncover variations in travel patterns across different areas and time types.\n\n\nWeekday Analysis\n\nFare: The average fare remains moderate, with Cluster 3 showing the lowest average fare at $17.99, reflecting a dominance of short-distance commuting trips. In contrast, Cluster 2 has a higher average fare of $49.48, suggesting longer, inter-district trips.\nTrip Miles: Distances vary significantly across clusters. Cluster 2 covers the longest distances, averaging 14.44 miles, indicative of inter-district commuting. Cluster 3, with 4.39 miles, highlights short-distance urban travel patterns.\nDuration: Average trip durations follow the distance patterns, with Cluster 2 taking the longest at 35.57 minutes and Cluster 3 being the shortest at 16.12 minutes, emphasizing the efficiency of weekday commutes.\nTrip Count: The highest trip volumes are recorded in Cluster 4 (5,611,508 trips) and Cluster 3 (2,701,924 trips), reflecting peak-hour commuting demand in urban areas.\n\nWeekend Analysis\n\nFare: Average fares slightly increase compared to weekdays, reflecting higher demand for leisure and entertainment trips. Cluster 4 has a fare of $23.08, while Cluster 3 averages $18.34.\nTrip Miles: Distances are generally shorter during weekends, with Cluster 4 covering an average of 4.35 miles, consistent with localized leisure activities. However, Cluster 2 covers longer distances, averaging 14.50 miles, suggesting inter-district travel.\nDuration: Travel times are longer, with Cluster 2 averaging 31.83 minutes, indicating more relaxed travel patterns for leisure activities. Shorter trips, such as those in Cluster 3 (15.66 minutes), highlight quick, intra-city movements.\nTrip Count: Although total trip volumes decrease compared to weekdays, Cluster 4 maintains a high volume of 1,513,730 trips, suggesting steady demand in central areas during weekends.",
    "crumbs": [
      "Weekday vs. Weekend Travel Patterns"
    ]
  },
  {
    "objectID": "Cluster.html#weekday-vs.-weekend",
    "href": "Cluster.html#weekday-vs.-weekend",
    "title": "Weekday vs. Weekend Travel Patterns",
    "section": "Weekday vs. Weekend",
    "text": "Weekday vs. Weekend\n\n\nWeekday: On weekdays, Uber trips in NYC are dominated by short-distance, high-frequency commuting patterns, particularly in Manhattan and central business districts. Clusters 3 and 4 capture the highest trip volumes, reflecting commuter demand during peak hours. Meanwhile, peripheral areas like Staten Island and the Bronx (Cluster 2) show longer-distance trips with higher fares, likely serving inter-district commuters traveling to and from residential neighborhoods.\nWeekend: During weekends, Uber trips shift towards leisure and recreational travel, characterized by higher fares per mile and longer trip durations. Cluster 4 highlights shorter-distance trips within city limits, focusing on entertainment and social activities. Conversely, Cluster 2 continues to reflect longer-distance travel patterns, potentially for day trips or special events, while local areas (Cluster 3) sustain moderate volumes for errands and social gatherings.\nThis spatial distribution difference highlights the shift in urban activity patterns between weekdays and weekends.",
    "crumbs": [
      "Weekday vs. Weekend Travel Patterns"
    ]
  },
  {
    "objectID": "prediction.html#data-preparation",
    "href": "prediction.html#data-preparation",
    "title": "Weekday vs. Weekend Travel Patterns",
    "section": "Data Preparation",
    "text": "Data Preparation\nBuilding on the observation in Part 1 that there are significant differences in pick-up time periods between weekdays and weekends, I refined the scope of analysis by focusing on Uber trip data from January 2023. To investigate potential differences in travel patterns, I first filtered the January data from the full 2023 dataset and categorized trips into weekdays and weekends based on the time_type.\nThis classification sets the foundation for clustering analysis, which examines key trip characteristics—including trip miles, fare, trip duration, and cost per mile—to provide deeper insights into how travel behaviors vary between weekdays and weekends.",
    "crumbs": [
      "Weekday vs. Weekend Travel Patterns"
    ]
  },
  {
    "objectID": "prediction.html#clustering-analysis-kmeans",
    "href": "prediction.html#clustering-analysis-kmeans",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Clustering Analysis (KMeans)",
    "text": "Clustering Analysis (KMeans)\nIn the Uber dataset, I aggregate by pickup_location and time_type then extract relevant statistical features.\nFirst, I aggregate the data by pickup_location and time_type, and select fare, trip_miles, trip_duration_min, and cost_per_mile as key features. I then calculate the average values of these features along with the total count for each zone.\nNext, a K-means clustering analysis is performed, dividing the data into 5 distinct clusters to uncover variations in travel patterns across different areas and time types.\n\n\nWeekday Analysis\n\nFare: The average fare remains moderate, with Cluster 3 showing the lowest average fare at $17.99, reflecting a dominance of short-distance commuting trips. In contrast, Cluster 2 has a higher average fare of $49.48, suggesting longer, inter-district trips.\nTrip Miles: Distances vary significantly across clusters. Cluster 2 covers the longest distances, averaging 14.44 miles, indicative of inter-district commuting. Cluster 3, with 4.39 miles, highlights short-distance urban travel patterns.\nDuration: Average trip durations follow the distance patterns, with Cluster 2 taking the longest at 35.57 minutes and Cluster 3 being the shortest at 16.12 minutes, emphasizing the efficiency of weekday commutes.\nTrip Count: The highest trip volumes are recorded in Cluster 4 (5,611,508 trips) and Cluster 3 (2,701,924 trips), reflecting peak-hour commuting demand in urban areas.\n\nWeekend Analysis\n\nFare: Average fares slightly increase compared to weekdays, reflecting higher demand for leisure and entertainment trips. Cluster 4 has a fare of $23.08, while Cluster 3 averages $18.34.\nTrip Miles: Distances are generally shorter during weekends, with Cluster 4 covering an average of 4.35 miles, consistent with localized leisure activities. However, Cluster 2 covers longer distances, averaging 14.50 miles, suggesting inter-district travel.\nDuration: Travel times are longer, with Cluster 2 averaging 31.83 minutes, indicating more relaxed travel patterns for leisure activities. Shorter trips, such as those in Cluster 3 (15.66 minutes), highlight quick, intra-city movements.\nTrip Count: Although total trip volumes decrease compared to weekdays, Cluster 4 maintains a high volume of 1,513,730 trips, suggesting steady demand in central areas during weekends.",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "prediction.html#weekday-vs.-weekend",
    "href": "prediction.html#weekday-vs.-weekend",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Weekday vs. Weekend",
    "text": "Weekday vs. Weekend\n\n\nWeekday: On weekdays, Uber trips in NYC are dominated by short-distance, high-frequency commuting patterns, particularly in Manhattan and central business districts. Clusters 3 and 4 capture the highest trip volumes, reflecting commuter demand during peak hours. Meanwhile, peripheral areas like Staten Island and the Bronx (Cluster 2) show longer-distance trips with higher fares, likely serving inter-district commuters traveling to and from residential neighborhoods.\nWeekend: During weekends, Uber trips shift towards leisure and recreational travel, characterized by higher fares per mile and longer trip durations. Cluster 4 highlights shorter-distance trips within city limits, focusing on entertainment and social activities. Conversely, Cluster 2 continues to reflect longer-distance travel patterns, potentially for day trips or special events, while local areas (Cluster 3) sustain moderate volumes for errands and social gatherings.\nThis spatial distribution difference highlights the shift in urban activity patterns between weekdays and weekends.",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "prediction.html",
    "href": "prediction.html",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "",
    "text": "Based on the analysis in Part 2, which revealed significant differences in travel patterns between weekdays and weekends, I will now develop a machine learning model to predict Uber demand across different zones. This prediction aims to assist drivers in identifying high-demand locations more effectively. The final dataset contains 48,135 rows",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "prediction.html#identify-zones-with-the-ighest-uber-trip-volumes",
    "href": "prediction.html#identify-zones-with-the-ighest-uber-trip-volumes",
    "title": "Weekday vs. Weekend Travel Patterns",
    "section": "Identify Zones with the ighest Uber trip volumes",
    "text": "Identify Zones with the ighest Uber trip volumes\nBuilding on the observation in Part 1 that there are significant differences in pick-up time periods between weekdays and weekends, I refined the scope of analysis by focusing on Uber trip data from January 2023. To investigate potential differences in travel patterns, I first filtered the January data from the full 2023 dataset and categorized trips into weekdays and weekends based on the time_type.\nThis classification sets the foundation for clustering analysis, which examines key trip characteristics—including trip miles, fare, trip duration, and cost per mile—to provide deeper insights into how travel behaviors vary between weekdays and weekends.",
    "crumbs": [
      "Weekday vs. Weekend Travel Patterns"
    ]
  },
  {
    "objectID": "prediction.html#identify-zones-with-the-highest-uber-trips",
    "href": "prediction.html#identify-zones-with-the-highest-uber-trips",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Identify Zones with the Highest Uber Trips",
    "text": "Identify Zones with the Highest Uber Trips\nTo gain insights into the distribution of Uber trip volumes across New York City, I visualized trips based on their geographic locations. \nThis map highlights the spatial distribution of Uber demand in different areas of New York City. The highest demand is concentrated in Manhattan’s core, parts of Brooklyn, and around JFK Airport—particularly in areas with dense commercial and entertainment activities. In contrast, lower demand is observed in peripheral regions, such as eastern Queens and Staten Island. Building on these observations, I will focus my subsequent analysis and demand forecasting on the Manhattan area.",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "Cluster.html#weekday-vs.-weekendaltair",
    "href": "Cluster.html#weekday-vs.-weekendaltair",
    "title": "Weekday vs. Weekend Travel Patterns",
    "section": "Weekday vs. Weekend（Altair）",
    "text": "Weekday vs. Weekend（Altair）\n\n\nWeekday: On weekdays, Uber trips in NYC are dominated by short-distance, high-frequency commuting patterns, particularly in Manhattan and central business districts. Clusters 3 and 4 capture the highest trip volumes, reflecting commuter demand during peak hours. Meanwhile, peripheral areas like Staten Island and the Bronx (Cluster 2) show longer-distance trips with higher fares, likely serving inter-district commuters traveling to and from residential neighborhoods.\nWeekend: During weekends, Uber trips shift towards leisure and recreational travel, characterized by higher fares per mile and longer trip durations. Cluster 4 highlights shorter-distance trips within city limits, focusing on entertainment and social activities. Conversely, Cluster 2 continues to reflect longer-distance travel patterns, potentially for day trips or special events, while local areas (Cluster 3) sustain moderate volumes for errands and social gatherings.\nThis spatial distribution difference highlights the shift in urban activity patterns between weekdays and weekends.",
    "crumbs": [
      "Weekday vs. Weekend Travel Patterns"
    ]
  },
  {
    "objectID": "prediction.html#feature-engineering",
    "href": "prediction.html#feature-engineering",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nI selected several potential variables that might influence Uber demand. These include facility data, such as restaurants, shops, subway stations, bus stops, schools, and hospitals, as well as demographic data, including median income, poverty population, and commuters by taxi. The data were sourced from OpenStreetMap and the United States Census Bureau.\nBelow, I engineered the features and show the restaurant_density, shop_density, subway_density, hospital_density, school_density, bus_stop_density in the Manhattan area. \nNext, query the Census API to get the demographic features, and visualize the distribution maps for taxi_commuters, median_income, and poverty_population. \n\n\nThen, I calculated the demand at each pickup location with a 1-day, 1-hour, and 2-hour lag, capturing short-term and daily variations in demand trends. These lag features help analysis the temporal patterns of Uber trip demand in Manhattan.\nLast, a correlation matrix was created to examined the correlation between those numerical variables. \n`",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "prediction.html#prediction-modeling-random-forest",
    "href": "prediction.html#prediction-modeling-random-forest",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Prediction Modeling (Random Forest)",
    "text": "Prediction Modeling (Random Forest)\n在相关性分析后，我使用了 a 70/30% \nWeekday: On weekdays, Uber trips in NYC are dominated by short-distance, high-frequency commuting patterns, particularly in Manhattan and central business districts. Clusters 3 and 4 capture the highest trip volumes, reflecting commuter demand during peak hours. Meanwhile, peripheral areas like Staten Island and the Bronx (Cluster 2) show longer-distance trips with higher fares, likely serving inter-district commuters traveling to and from residential neighborhoods.\nWeekend: During weekends, Uber trips shift towards leisure and recreational travel, characterized by higher fares per mile and longer trip durations. Cluster 4 highlights shorter-distance trips within city limits, focusing on entertainment and social activities. Conversely, Cluster 2 continues to reflect longer-distance travel patterns, potentially for day trips or special events, while local areas (Cluster 3) sustain moderate volumes for errands and social gatherings.\nThis spatial distribution difference highlights the shift in urban activity patterns between weekdays and weekends.",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "prediction.html#random-forest-based-prediction-modeling",
    "href": "prediction.html#random-forest-based-prediction-modeling",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Random Forest-Based Prediction Modeling",
    "text": "Random Forest-Based Prediction Modeling\nAfter conducting a correlation analysis, I split the data into a 70/30% training/test set and developed a Random Forest model.\nThe model achieved a Mean Absolute Error (MAE) of 31.05 and a Test R-squared of 0.90, indicating that it can serve as a relatively reliable predictive model.\n\nMAE = 31.046883748972288\n\n\nTest R-squared = 0.8960920639191985\n\nTo further analyze the results, I plotted a bar chart of feature importance, which revealed that the top three most influential features are cost_per_mile, median_income, and shop_density.",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  },
  {
    "objectID": "prediction.html#validation",
    "href": "prediction.html#validation",
    "title": "Random Forest-Based Uber Trips Prediction",
    "section": "Validation",
    "text": "Validation\nFor the test set, we calculated the predicted count, percent error as well as absolute percent error for each observation.\n \nI visualized the spatial distribution of percentage errors and compared the spatial patterns of actual and predicted demand counts. This predictive model effectively captures the overall demand trends across the zone. \nThe maps illustrate the model’s performance in predicting demand across Manhattan. The first map highlights percentage errors, revealing regions with over-predictions (red) and under-predictions (blue). While most areas show relatively small errors, a few zones, particularly in the southern and northern parts, exhibit larger deviations, suggesting localized inaccuracies.\nThe second and third maps compare the actual and predicted counts, showing that the model effectively captures the overall spatial demand patterns, particularly in high-demand zones. However, discrepancies in certain areas, as reflected by the error map, indicate that the model may struggle with fine-scale variations.\nOverall, the model demonstrates strong predictive performance for general trends but requires adjustments to improve accuracy in specific regions with higher errors.",
    "crumbs": [
      "Random Forest-Based Uber Trips Prediction"
    ]
  }
]